{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebMmQhLGo8NF",
        "outputId": "55ac6c6e-0fba-4a8d-9022-3183b6f28f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/vishwaraghava009/Human_TalkingHead.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjAgEM0j68zT",
        "outputId": "ed7f8bae-aa01-4deb-d23a-bcf5845c1da7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Human_TalkingHead'...\n",
            "remote: Enumerating objects: 192, done.\u001b[K\n",
            "remote: Counting objects: 100% (192/192), done.\u001b[K\n",
            "remote: Compressing objects: 100% (133/133), done.\u001b[K\n",
            "remote: Total 192 (delta 54), reused 182 (delta 44), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (192/192), 34.07 MiB | 14.30 MiB/s, done.\n",
            "Resolving deltas: 100% (54/54), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Human_TalkingHead"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5CrG1DB7B4m",
        "outputId": "292c62ce-c7ed-4b8d-8181-ac00f772e718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Human_TalkingHead\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "git clone https://github.com/yl4579/StyleTTS2.git\n",
        "cd StyleTTS2\n",
        "pip install SoundFile torchaudio munch torch pydub pyyaml librosa nltk matplotlib accelerate transformers phonemizer einops einops-exts tqdm typing-extensions git+https://github.com/resemble-ai/monotonic_align.git\n",
        "sudo apt-get install espeak-ng\n",
        "git-lfs clone https://huggingface.co/yl4579/StyleTTS2-LJSpeech\n",
        "mv StyleTTS2-LJSpeech/Models ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02vyud3jBdFX",
        "outputId": "e6f73bda-934c-4007-de56-f3d0b6d66181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'StyleTTS2'...\n",
            "remote: Enumerating objects: 372, done.\u001b[K\n",
            "remote: Counting objects: 100% (245/245), done.\u001b[K\n",
            "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
            "remote: Total 372 (delta 201), reused 142 (delta 142), pack-reused 127\u001b[K\n",
            "Receiving objects: 100% (372/372), 133.96 MiB | 14.78 MiB/s, done.\n",
            "Resolving deltas: 100% (206/206), done.\n",
            "Updating files: 100% (48/48), done.\n",
            "Collecting git+https://github.com/resemble-ai/monotonic_align.git\n",
            "  Cloning https://github.com/resemble-ai/monotonic_align.git to /tmp/pip-req-build-fvpri39z\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/resemble-ai/monotonic_align.git /tmp/pip-req-build-fvpri39z\n",
            "  Resolved https://github.com/resemble-ai/monotonic_align.git to commit 78b985be210a03d08bc3acc01c4df0442105366f\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: SoundFile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Collecting munch\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Collecting phonemizer\n",
            "  Downloading phonemizer-3.2.1-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops-exts\n",
            "  Downloading einops_exts-0.0.4-py3-none-any.whl (3.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (4.12.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from SoundFile) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.58.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Collecting segments (from phonemizer)\n",
            "  Downloading segments-2.2.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: attrs>=18.1 in /usr/local/lib/python3.10/dist-packages (from phonemizer) (23.2.0)\n",
            "Collecting dlinfo (from phonemizer)\n",
            "  Downloading dlinfo-1.2.1-py3-none-any.whl (3.6 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->SoundFile) (2.22)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Collecting clldutils>=1.7.3 (from segments->phonemizer)\n",
            "  Downloading clldutils-3.22.2-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting csvw>=1.5.6 (from segments->phonemizer)\n",
            "  Downloading csvw-3.3.0-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (0.9.0)\n",
            "Collecting colorlog (from clldutils>=1.7.3->segments->phonemizer)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Collecting bibtexparser>=2.0.0b4 (from clldutils>=1.7.3->segments->phonemizer)\n",
            "  Downloading bibtexparser-2.0.0b7-py3-none-any.whl (38 kB)\n",
            "Collecting pylatexenc (from clldutils>=1.7.3->segments->phonemizer)\n",
            "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (3.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (4.9.4)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (2.15.0)\n",
            "Collecting colorama (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting isodate (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (4.19.2)\n",
            "Collecting language-tags (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Downloading language_tags-1.2.0-py3-none-any.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.4/213.4 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rdflib (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2 (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: uritemplate>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (4.1.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (0.18.1)\n",
            "Building wheels for collected packages: monotonic_align, pylatexenc\n",
            "  Building wheel for monotonic_align (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for monotonic_align: filename=monotonic_align-1.2-cp310-cp310-linux_x86_64.whl size=1419424 sha256=c30127d7e48517aefac0376d0eb0d4f7ee4465ff5906c83c863ba1e3391a471a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cvi10zy3/wheels/62/6e/e2/2758abe7d7bf44cf150d943471be9836770e7c40c1daf4b7bd\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136816 sha256=9178fa4899a25bf79c3edfc454ed6ca4f2e19e6d0fa2b2c4b0c2ec6362ed2d25\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/31/8b/e09b0386afd80cfc556c00408c9aeea5c35c4d484a9c762fd5\n",
            "Successfully built monotonic_align pylatexenc\n",
            "Installing collected packages: rfc3986, pylatexenc, pydub, language-tags, dlinfo, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, munch, monotonic_align, isodate, einops, colorlog, colorama, bibtexparser, rdflib, nvidia-cusparse-cu12, nvidia-cudnn-cu12, einops-exts, clldutils, nvidia-cusolver-cu12, csvw, segments, accelerate, phonemizer\n",
            "Successfully installed accelerate-0.31.0 bibtexparser-2.0.0b7 clldutils-3.22.2 colorama-0.4.6 colorlog-6.8.2 csvw-3.3.0 dlinfo-1.2.1 einops-0.8.0 einops-exts-0.0.4 isodate-0.6.1 language-tags-1.2.0 monotonic_align-1.2 munch-4.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 phonemizer-3.2.1 pydub-0.25.1 pylatexenc-2.10 rdflib-7.0.0 rfc3986-1.5.0 segments-2.2.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  espeak-ng-data libespeak-ng1 libpcaudio0 libsonic0\n",
            "The following NEW packages will be installed:\n",
            "  espeak-ng espeak-ng-data libespeak-ng1 libpcaudio0 libsonic0\n",
            "0 upgraded, 5 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 4,525 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpcaudio0 amd64 1.1-6build2 [8,956 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsonic0 amd64 0.2.0-11build1 [10.3 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 espeak-ng-data amd64 1.50+dfsg-10 [3,956 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libespeak-ng1 amd64 1.50+dfsg-10 [207 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 espeak-ng amd64 1.50+dfsg-10 [343 kB]\n",
            "Fetched 4,525 kB in 2s (1,908 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 5.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpcaudio0:amd64.\n",
            "(Reading database ... 121913 files and directories currently installed.)\n",
            "Preparing to unpack .../libpcaudio0_1.1-6build2_amd64.deb ...\n",
            "Unpacking libpcaudio0:amd64 (1.1-6build2) ...\n",
            "Selecting previously unselected package libsonic0:amd64.\n",
            "Preparing to unpack .../libsonic0_0.2.0-11build1_amd64.deb ...\n",
            "Unpacking libsonic0:amd64 (0.2.0-11build1) ...\n",
            "Selecting previously unselected package espeak-ng-data:amd64.\n",
            "Preparing to unpack .../espeak-ng-data_1.50+dfsg-10_amd64.deb ...\n",
            "Unpacking espeak-ng-data:amd64 (1.50+dfsg-10) ...\n",
            "Selecting previously unselected package libespeak-ng1:amd64.\n",
            "Preparing to unpack .../libespeak-ng1_1.50+dfsg-10_amd64.deb ...\n",
            "Unpacking libespeak-ng1:amd64 (1.50+dfsg-10) ...\n",
            "Selecting previously unselected package espeak-ng.\n",
            "Preparing to unpack .../espeak-ng_1.50+dfsg-10_amd64.deb ...\n",
            "Unpacking espeak-ng (1.50+dfsg-10) ...\n",
            "Setting up libpcaudio0:amd64 (1.1-6build2) ...\n",
            "Setting up libsonic0:amd64 (0.2.0-11build1) ...\n",
            "Setting up espeak-ng-data:amd64 (1.50+dfsg-10) ...\n",
            "Setting up libespeak-ng1:amd64 (1.50+dfsg-10) ...\n",
            "Setting up espeak-ng (1.50+dfsg-10) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "WARNING: 'git lfs clone' is deprecated and will not be updated\n",
            "          with new flags from 'git clone'\n",
            "\n",
            "'git clone' has been updated in upstream Git to have comparable\n",
            "speeds to 'git lfs clone'.\n",
            "Cloning into 'StyleTTS2-LJSpeech'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Total 20 (delta 0), reused 0 (delta 0), pack-reused 20 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (20/20), 3.08 KiB | 631.00 KiB/s, done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/wyhsirius/LIA.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ra0DplH3mFPG",
        "outputId": "b2bc9d61-230a-4b41-d718-1d1b6321685c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LIA'...\n",
            "remote: Enumerating objects: 112, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 112 (delta 25), reused 16 (delta 15), pack-reused 73\u001b[K\n",
            "Receiving objects: 100% (112/112), 21.55 MiB | 7.32 MiB/s, done.\n",
            "Resolving deltas: 100% (34/34), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install django\n",
        "!pip install groq\n",
        "!pip install python-dotenv\n",
        "!pip install djangorestframework\n",
        "!pip install numpy\n",
        "!pip install opencv-python\n",
        "!pip install dlib\n",
        "!pip install av\n",
        "!pip install deepface\n",
        "!pip install pyAudioAnalysis\n",
        "!pip install pyaudio\n",
        "!pip install librosa\n",
        "!pip install phonemizer\n",
        "!pip install pydub\n",
        "!pip install munch\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install torchaudio\n",
        "!pip install espeakng\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install localtunnel\n",
        "!pip install numpy scipy matplotlib\n",
        "!pip install git+https://github.com/tyiannak/pyAudioAnalysis.git\n",
        "!pip install watchdog"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tn_c9XKA7xPh",
        "outputId": "2f224b98-670a-4d57-fb85-629f41027c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting django\n",
            "  Downloading Django-5.0.6-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: asgiref, django\n",
            "Successfully installed asgiref-3.8.1 django-5.0.6\n",
            "Collecting groq\n",
            "  Downloading groq-0.9.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.7.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.18.4)\n",
            "Installing collected packages: h11, httpcore, httpx, groq\n",
            "Successfully installed groq-0.9.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n",
            "Collecting djangorestframework\n",
            "  Downloading djangorestframework-3.15.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: django>=3.0 in /usr/local/lib/python3.10/dist-packages (from djangorestframework) (5.0.6)\n",
            "Requirement already satisfied: asgiref<4,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from django>=3.0->djangorestframework) (3.8.1)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from django>=3.0->djangorestframework) (0.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from asgiref<4,>=3.7.0->django>=3.0->djangorestframework) (4.12.2)\n",
            "Installing collected packages: djangorestframework\n",
            "Successfully installed djangorestframework-3.15.1\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n",
            "Requirement already satisfied: dlib in /usr/local/lib/python3.10/dist-packages (19.24.4)\n",
            "Collecting av\n",
            "  Downloading av-12.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-12.1.0\n",
            "Collecting deepface\n",
            "  Downloading deepface-0.0.92-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.5/105.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.31.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.0.3)\n",
            "Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from deepface) (5.1.0)\n",
            "Requirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (4.66.4)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (9.4.0)\n",
            "Requirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.10/dist-packages (from deepface) (4.8.0.76)\n",
            "Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.15.0)\n",
            "Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.15.0)\n",
            "Requirement already satisfied: Flask>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.2.5)\n",
            "Collecting mtcnn>=0.1.0 (from deepface)\n",
            "  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting retina-face>=0.0.1 (from deepface)\n",
            "  Downloading retina_face-0.0.17-py3-none-any.whl (25 kB)\n",
            "Collecting fire>=0.4.0 (from deepface)\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gunicorn>=20.1.0 (from deepface)\n",
            "  Downloading gunicorn-22.0.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire>=0.4.0->deepface) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire>=0.4.0->deepface) (2.4.0)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (3.0.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (8.1.7)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->deepface) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->deepface) (3.14.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gunicorn>=20.1.0->deepface) (24.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->deepface) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->deepface) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->deepface) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (2024.6.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface) (0.43.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=1.1.2->deepface) (2.1.5)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (0.7.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (1.7.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (3.2.2)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117029 sha256=89908bc17168048aa8b824bf089cc9b20a1bd9f3d8e533492f40f480e5eb3342\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "Successfully built fire\n",
            "Installing collected packages: gunicorn, fire, mtcnn, retina-face, deepface\n",
            "Successfully installed deepface-0.0.92 fire-0.6.0 gunicorn-22.0.0 mtcnn-0.1.1 retina-face-0.0.17\n",
            "Collecting pyAudioAnalysis\n",
            "  Downloading pyAudioAnalysis-0.3.14.tar.gz (41.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pyAudioAnalysis\n",
            "  Building wheel for pyAudioAnalysis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyAudioAnalysis: filename=pyAudioAnalysis-0.3.14-py3-none-any.whl size=41264373 sha256=65f058944f879613bdd77a7b969935a07c40a131b867b72023a96dc7591600e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/54/73/fa830689c2440d2c81ff175c60e374930ad1607a8881e0f43f\n",
            "Successfully built pyAudioAnalysis\n",
            "Installing collected packages: pyAudioAnalysis\n",
            "Successfully installed pyAudioAnalysis-0.3.14\n",
            "Collecting pyaudio\n",
            "  Downloading PyAudio-0.2.14.tar.gz (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pyaudio\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for pyaudio \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pyaudio (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for pyaudio\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build pyaudio\n",
            "\u001b[31mERROR: Could not build wheels for pyaudio, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.58.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa) (24.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.31.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.6.2)\n",
            "Requirement already satisfied: phonemizer in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from phonemizer) (1.4.2)\n",
            "Requirement already satisfied: segments in /usr/local/lib/python3.10/dist-packages (from phonemizer) (2.2.1)\n",
            "Requirement already satisfied: attrs>=18.1 in /usr/local/lib/python3.10/dist-packages (from phonemizer) (23.2.0)\n",
            "Requirement already satisfied: dlinfo in /usr/local/lib/python3.10/dist-packages (from phonemizer) (1.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from phonemizer) (4.12.2)\n",
            "Requirement already satisfied: clldutils>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from segments->phonemizer) (3.22.2)\n",
            "Requirement already satisfied: csvw>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from segments->phonemizer) (3.3.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from segments->phonemizer) (2024.5.15)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (0.9.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (6.8.2)\n",
            "Requirement already satisfied: bibtexparser>=2.0.0b4 in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (2.0.0b7)\n",
            "Requirement already satisfied: pylatexenc in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (2.10)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (3.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (4.9.4)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (2.1.5)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (2.15.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (0.4.6)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (0.6.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (4.19.2)\n",
            "Requirement already satisfied: language-tags in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (1.2.0)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (7.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (2.31.0)\n",
            "Requirement already satisfied: rfc3986<2 in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (1.5.0)\n",
            "Requirement already satisfied: uritemplate>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (4.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate->csvw>=1.5.6->segments->phonemizer) (1.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (0.18.1)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib->csvw>=1.5.6->segments->phonemizer) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (2024.6.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.10/dist-packages (4.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.31.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchaudio) (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->torchaudio) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0->torchaudio) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0->torchaudio) (1.3.0)\n",
            "Collecting espeakng\n",
            "  Downloading espeakng-1.0.3-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: espeakng\n",
            "Successfully installed espeakng-1.0.3\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement localtunnel (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for localtunnel\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Collecting git+https://github.com/tyiannak/pyAudioAnalysis.git\n",
            "  Cloning https://github.com/tyiannak/pyAudioAnalysis.git to /tmp/pip-req-build-ti0pix3r\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/tyiannak/pyAudioAnalysis.git /tmp/pip-req-build-ti0pix3r\n",
            "  Resolved https://github.com/tyiannak/pyAudioAnalysis.git to commit ae00ebd446d497dbbb654c40a71810e7fdf2cc1b\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=3.4.2 in /usr/local/lib/python3.10/dist-packages (from pyAudioAnalysis==0.3.14) (3.7.1)\n",
            "Collecting simplejson>=3.16.0 (from pyAudioAnalysis==0.3.14)\n",
            "  Downloading simplejson-3.19.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from pyAudioAnalysis==0.3.14) (1.11.4)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from pyAudioAnalysis==0.3.14) (1.25.2)\n",
            "Collecting hmmlearn>=0.2.5 (from pyAudioAnalysis==0.3.14)\n",
            "  Downloading hmmlearn-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.1/161.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting eyeD3>=0.9.6 (from pyAudioAnalysis==0.3.14)\n",
            "  Downloading eyed3-0.9.7-py3-none-any.whl (246 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.1/246.1 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from pyAudioAnalysis==0.3.14) (0.25.1)\n",
            "Requirement already satisfied: scikit_learn>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from pyAudioAnalysis==0.3.14) (1.2.2)\n",
            "Requirement already satisfied: tqdm>=4.52.0 in /usr/local/lib/python3.10/dist-packages (from pyAudioAnalysis==0.3.14) (4.66.4)\n",
            "Requirement already satisfied: plotly>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from pyAudioAnalysis==0.3.14) (5.15.0)\n",
            "Requirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.10/dist-packages (from pyAudioAnalysis==0.3.14) (2.0.3)\n",
            "Collecting imblearn (from pyAudioAnalysis==0.3.14)\n",
            "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
            "Collecting coverage[toml]<6.0.0,>=5.3.1 (from eyeD3>=0.9.6->pyAudioAnalysis==0.3.14)\n",
            "  Downloading coverage-5.5-cp310-cp310-manylinux1_x86_64.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.0/239.0 kB\u001b[0m \u001b[31m867.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deprecation<3.0.0,>=2.1.0 (from eyeD3>=0.9.6->pyAudioAnalysis==0.3.14)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.0.7 (from eyeD3>=0.9.6->pyAudioAnalysis==0.3.14)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.2->pyAudioAnalysis==0.3.14) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.2->pyAudioAnalysis==0.3.14) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.2->pyAudioAnalysis==0.3.14) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.2->pyAudioAnalysis==0.3.14) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.2->pyAudioAnalysis==0.3.14) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.2->pyAudioAnalysis==0.3.14) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.2->pyAudioAnalysis==0.3.14) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.2->pyAudioAnalysis==0.3.14) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pyAudioAnalysis==0.3.14) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pyAudioAnalysis==0.3.14) (2024.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.3.1->pyAudioAnalysis==0.3.14) (8.3.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>=0.24.2->pyAudioAnalysis==0.3.14) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>=0.24.2->pyAudioAnalysis==0.3.14) (3.5.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (from imblearn->pyAudioAnalysis==0.3.14) (0.10.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from coverage[toml]<6.0.0,>=5.3.1->eyeD3>=0.9.6->pyAudioAnalysis==0.3.14) (0.10.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.4.2->pyAudioAnalysis==0.3.14) (1.16.0)\n",
            "Installing collected packages: filetype, simplejson, deprecation, coverage, hmmlearn, eyeD3, imblearn\n",
            "Successfully installed coverage-5.5 deprecation-2.1.0 eyeD3-0.9.7 filetype-1.2.0 hmmlearn-0.3.2 imblearn-0.0 simplejson-3.19.2\n",
            "Collecting watchdog\n",
            "  Downloading watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog\n",
            "Successfully installed watchdog-4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y portaudio19-dev\n",
        "!pip install pyaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiGTuxP_ARrJ",
        "outputId": "7f09916a-966a-40b9-b74d-306e06101091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libportaudio2 libportaudiocpp0\n",
            "Suggested packages:\n",
            "  portaudio19-doc\n",
            "The following NEW packages will be installed:\n",
            "  libportaudio2 libportaudiocpp0 portaudio19-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 188 kB of archives.\n",
            "After this operation, 927 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudio2 amd64 19.6.0-1.1 [65.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudiocpp0 amd64 19.6.0-1.1 [16.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 portaudio19-dev amd64 19.6.0-1.1 [106 kB]\n",
            "Fetched 188 kB in 2s (123 kB/s)\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 122454 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package libportaudiocpp0:amd64.\n",
            "Preparing to unpack .../libportaudiocpp0_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package portaudio19-dev:amd64.\n",
            "Preparing to unpack .../portaudio19-dev_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Setting up portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "Collecting pyaudio\n",
            "  Using cached PyAudio-0.2.14.tar.gz (47 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pyaudio\n",
            "  Building wheel for pyaudio (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyaudio: filename=PyAudio-0.2.14-cp310-cp310-linux_x86_64.whl size=63864 sha256=4cd368e1b46658244d46c158caed1df52f843307bd68dabbc1f8635d1d7e3e50\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/21/f4/0b51d41ba79e51b16295cbb096ec49f334792814d545b508c5\n",
            "Successfully built pyaudio\n",
            "Installing collected packages: pyaudio\n",
            "Successfully installed pyaudio-0.2.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SpeechRecognition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9spcmC9AYPx",
        "outputId": "52e7a0c5-217b-4bed-f4d5-ce9a3433b2c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.10.4-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2024.6.2)\n",
            "Installing collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.10.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/TMElyralab/MuseTalk.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wi4DOBRQVN2B",
        "outputId": "a254136d-9933-4eae-f2f3-5a88cb16e152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MuseTalk'...\n",
            "remote: Enumerating objects: 249, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 249 (delta 30), reused 21 (delta 21), pack-reused 204\u001b[K\n",
            "Receiving objects: 100% (249/249), 23.92 MiB | 13.31 MiB/s, done.\n",
            "Resolving deltas: 100% (69/69), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd MuseTalk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qp-5AEgQXQPu",
        "outputId": "6b2dec0e-1b8d-4c6b-ab1c-fe267b4479d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Human_TalkingHead/MuseTalk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install --no-cache-dir -U openmim\n",
        "!mim install mmengine\n",
        "!mim install \"mmcv>=2.0.1\"\n",
        "!mim install \"mmdet>=3.1.0\"\n",
        "!mim install \"mmpose>=1.1.0\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wmVzA3vtVS34",
        "outputId": "6f8b9ec5-e7a5-4be4-ad39-fcf512646155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.0.1 (from -r requirements.txt (line 2))\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp310-cp310-linux_x86_64.whl (2267.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m417.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.15.2 (from -r requirements.txt (line 3))\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.15.2%2Bcu118-cp310-cp310-linux_x86_64.whl (6.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.0.2 (from -r requirements.txt (line 4))\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.0.2%2Bcu118-cp310-cp310-linux_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diffusers==0.27.2 (from -r requirements.txt (line 5))\n",
            "  Downloading diffusers-0.27.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate==0.28.0 (from -r requirements.txt (line 6))\n",
            "  Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow==2.12.0 (from -r requirements.txt (line 7))\n",
            "  Downloading tensorflow-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard==2.12.0 (from -r requirements.txt (line 8))\n",
            "  Downloading tensorboard-2.12.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencv-python==4.9.0.80 (from -r requirements.txt (line 9))\n",
            "  Downloading opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soundfile==0.12.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.12.1)\n",
            "Collecting transformers==4.39.2 (from -r requirements.txt (line 11))\n",
            "  Downloading transformers-4.39.2-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (5.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (2.31.0)\n",
            "Requirement already satisfied: imageio[ffmpeg] in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (2.31.6)\n",
            "Collecting omegaconf (from -r requirements.txt (line 17))\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpeg-python (from -r requirements.txt (line 18))\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Collecting gradio (from -r requirements.txt (line 19))\n",
            "  Downloading gradio-4.36.1-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spaces (from -r requirements.txt (line 20))\n",
            "  Downloading spaces-0.28.3-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (1.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 2)) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 2)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 2)) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 2)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 2)) (3.1.4)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1->-r requirements.txt (line 2))\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2->-r requirements.txt (line 3)) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2->-r requirements.txt (line 3)) (9.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers==0.27.2->-r requirements.txt (line 5)) (7.1.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.2 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.27.2->-r requirements.txt (line 5)) (0.23.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.27.2->-r requirements.txt (line 5)) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.27.2->-r requirements.txt (line 5)) (0.4.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0->-r requirements.txt (line 6)) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0->-r requirements.txt (line 6)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0->-r requirements.txt (line 6)) (6.0.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 7)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 7)) (24.3.25)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12.0->-r requirements.txt (line 7))\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 7)) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 7)) (3.9.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 7)) (0.4.26)\n",
            "Collecting keras<2.13,>=2.12.0 (from tensorflow==2.12.0->-r requirements.txt (line 7))\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 7)) (18.1.1)\n",
            "Collecting numpy (from torchvision==0.15.2->-r requirements.txt (line 3))\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 7)) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 7)) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 7)) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 7)) (1.16.0)\n",
            "Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12.0->-r requirements.txt (line 7))\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 7)) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 7)) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->-r requirements.txt (line 7)) (0.37.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.12.0->-r requirements.txt (line 8)) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard==2.12.0->-r requirements.txt (line 8))\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.12.0->-r requirements.txt (line 8)) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.12.0->-r requirements.txt (line 8)) (0.7.2)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard==2.12.0->-r requirements.txt (line 8))\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.12.0->-r requirements.txt (line 8)) (3.0.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.12.0->-r requirements.txt (line 8)) (0.43.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile==0.12.1->-r requirements.txt (line 10)) (1.16.0)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.39.2->-r requirements.txt (line 11))\n",
            "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.2->-r requirements.txt (line 11)) (4.66.4)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 2)) (3.27.9)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 2))\n",
            "  Downloading lit-18.1.7-py3-none-any.whl (96 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 13)) (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 14)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 14)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 14)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 14)) (2024.6.2)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (from imageio[ffmpeg]->-r requirements.txt (line 15)) (0.5.1)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->-r requirements.txt (line 17))\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python->-r requirements.txt (line 18)) (0.18.3)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio->-r requirements.txt (line 19))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 19)) (4.2.2)\n",
            "Collecting fastapi (from gradio->-r requirements.txt (line 19))\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio->-r requirements.txt (line 19))\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==1.0.1 (from gradio->-r requirements.txt (line 19))\n",
            "  Downloading gradio_client-1.0.1-py3-none-any.whl (318 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.1/318.1 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 19)) (0.27.0)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 19)) (6.4.0)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 19)) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 19)) (3.7.1)\n",
            "Collecting orjson~=3.0 (from gradio->-r requirements.txt (line 19))\n",
            "  Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 19)) (2.0.3)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 19)) (2.7.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 19)) (0.25.1)\n",
            "Collecting python-multipart>=0.0.9 (from gradio->-r requirements.txt (line 19))\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio->-r requirements.txt (line 19))\n",
            "  Downloading ruff-0.4.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio->-r requirements.txt (line 19))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio->-r requirements.txt (line 19))\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 19)) (0.12.3)\n",
            "Collecting uvicorn>=0.14.0 (from gradio->-r requirements.txt (line 19))\n",
            "  Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.0.1->gradio->-r requirements.txt (line 19)) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==1.0.1->gradio->-r requirements.txt (line 19))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->-r requirements.txt (line 21)) (4.4.2)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->-r requirements.txt (line 21)) (0.1.10)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 19)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 19)) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 19)) (0.12.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile==0.12.1->-r requirements.txt (line 10)) (2.22)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.12.0->-r requirements.txt (line 8)) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.12.0->-r requirements.txt (line 8)) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.12.0->-r requirements.txt (line 8)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.12.0->-r requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio->-r requirements.txt (line 19)) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio->-r requirements.txt (line 19)) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio->-r requirements.txt (line 19)) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio->-r requirements.txt (line 19)) (0.14.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0->-r requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0->-r requirements.txt (line 7)) (1.11.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 19)) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 19)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 19)) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 19)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 19)) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 19)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 19)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 19)) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio->-r requirements.txt (line 19)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio->-r requirements.txt (line 19)) (2.18.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 19)) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 19)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 19)) (13.7.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown->-r requirements.txt (line 13)) (2.5)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio->-r requirements.txt (line 19))\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi->gradio->-r requirements.txt (line 19))\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->gradio->-r requirements.txt (line 19))\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi->gradio->-r requirements.txt (line 19))\n",
            "  Downloading email_validator-2.1.2-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers==0.27.2->-r requirements.txt (line 5)) (3.19.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 14)) (1.7.1)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->-r requirements.txt (line 2)) (1.3.0)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->gradio->-r requirements.txt (line 19))\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 19)) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 19)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 19)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 19)) (0.18.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.12.0->-r requirements.txt (line 8)) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.12.0->-r requirements.txt (line 8)) (3.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 19)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 19)) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio->-r requirements.txt (line 19)) (1.2.1)\n",
            "Collecting httptools>=0.5.0 (from uvicorn>=0.14.0->gradio->-r requirements.txt (line 19))\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio->-r requirements.txt (line 19)) (1.0.1)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn>=0.14.0->gradio->-r requirements.txt (line 19))\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn>=0.14.0->gradio->-r requirements.txt (line 19))\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 19)) (0.1.2)\n",
            "Building wheels for collected packages: antlr4-python3-runtime, ffmpy\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=9e263f94786994fd252be4a4c3c9fa973fe16dead85bc701a0f9398d6b50fb3a\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=eb73fa4d9630b78688db19bbaedcaa8bf9ec3df303c93fa7883bdba42cd02d51\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built antlr4-python3-runtime ffmpy\n",
            "Installing collected packages: tensorboard-plugin-wit, lit, ffmpy, antlr4-python3-runtime, websockets, uvloop, uvicorn, ujson, tomlkit, tensorflow-estimator, semantic-version, ruff, python-multipart, orjson, omegaconf, numpy, keras, httptools, gast, ffmpeg-python, dnspython, aiofiles, watchfiles, starlette, opencv-python, email_validator, tokenizers, gradio-client, google-auth-oauthlib, diffusers, transformers, tensorboard, fastapi-cli, tensorflow, fastapi, gradio, spaces, triton, torch, torchvision, torchaudio, accelerate\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.15.0\n",
            "    Uninstalling tensorflow-estimator-2.15.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.15.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.4\n",
            "    Uninstalling gast-0.5.4:\n",
            "      Successfully uninstalled gast-0.5.4\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.8.0.76\n",
            "    Uninstalling opencv-python-4.8.0.76:\n",
            "      Successfully uninstalled opencv-python-4.8.0.76\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.0\n",
            "    Uninstalling google-auth-oauthlib-1.2.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.2\n",
            "    Uninstalling transformers-4.41.2:\n",
            "      Successfully uninstalled transformers-4.41.2\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.0\n",
            "    Uninstalling triton-2.3.0:\n",
            "      Successfully uninstalled triton-2.3.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.0+cu121\n",
            "    Uninstalling torch-2.3.0+cu121:\n",
            "      Successfully uninstalled torch-2.3.0+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.18.0+cu121\n",
            "    Uninstalling torchvision-0.18.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.18.0+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.3.0+cu121\n",
            "    Uninstalling torchaudio-2.3.0+cu121:\n",
            "      Successfully uninstalled torchaudio-2.3.0+cu121\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.31.0\n",
            "    Uninstalling accelerate-0.31.0:\n",
            "      Successfully uninstalled accelerate-0.31.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.23.5 which is incompatible.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.12.0 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.0.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.28.0 aiofiles-23.2.1 antlr4-python3-runtime-4.9.3 diffusers-0.27.2 dnspython-2.6.1 email_validator-2.1.2 fastapi-0.111.0 fastapi-cli-0.0.4 ffmpeg-python-0.2.0 ffmpy-0.3.2 gast-0.4.0 google-auth-oauthlib-0.4.6 gradio-4.36.1 gradio-client-1.0.1 httptools-0.6.1 keras-2.12.0 lit-18.1.7 numpy-1.23.5 omegaconf-2.3.0 opencv-python-4.9.0.80 orjson-3.10.5 python-multipart-0.0.9 ruff-0.4.9 semantic-version-2.10.0 spaces-0.28.3 starlette-0.37.2 tensorboard-2.12.0 tensorboard-plugin-wit-1.8.1 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tokenizers-0.15.2 tomlkit-0.12.0 torch-2.0.1+cu118 torchaudio-2.0.2+cu118 torchvision-0.15.2+cu118 transformers-4.39.2 triton-2.0.0 ujson-5.10.0 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-11.0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pydevd_plugins"
                ]
              },
              "id": "aad97f29bd8c415388a030239c5d4f9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openmim\n",
            "  Downloading openmim-0.3.9-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/52.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click in /usr/local/lib/python3.10/dist-packages (from openmim) (8.1.7)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from openmim) (0.4.6)\n",
            "Collecting model-index (from openmim)\n",
            "  Downloading model_index-0.1.11-py3-none-any.whl (34 kB)\n",
            "Collecting opendatalab (from openmim)\n",
            "  Downloading opendatalab-0.0.10-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from openmim) (2.0.3)\n",
            "Requirement already satisfied: pip>=19.3 in /usr/local/lib/python3.10/dist-packages (from openmim) (23.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from openmim) (2.31.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from openmim) (13.7.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from openmim) (0.9.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from model-index->openmim) (6.0.1)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from model-index->openmim) (3.6)\n",
            "Collecting ordered-set (from model-index->openmim)\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Collecting pycryptodome (from opendatalab->openmim)\n",
            "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m129.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatalab->openmim) (4.66.4)\n",
            "Collecting openxlab (from opendatalab->openmim)\n",
            "  Downloading openxlab-0.1.0-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.4/307.4 kB\u001b[0m \u001b[31m356.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (1.23.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->openmim) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->openmim) (1.16.0)\n",
            "Requirement already satisfied: filelock~=3.14.0 in /usr/local/lib/python3.10/dist-packages (from openxlab->opendatalab->openmim) (3.14.0)\n",
            "Collecting oss2~=2.17.0 (from openxlab->opendatalab->openmim)\n",
            "  Downloading oss2-2.17.0.tar.gz (259 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m321.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging~=24.0 in /usr/local/lib/python3.10/dist-packages (from openxlab->opendatalab->openmim) (24.1)\n",
            "Collecting requests (from openmim)\n",
            "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m269.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rich (from openmim)\n",
            "  Downloading rich-13.4.2-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.4/239.4 kB\u001b[0m \u001b[31m288.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setuptools~=60.2.0 (from openxlab->opendatalab->openmim)\n",
            "  Downloading setuptools-60.2.0-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.1/953.1 kB\u001b[0m \u001b[31m329.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm (from opendatalab->openmim)\n",
            "  Downloading tqdm-4.65.2-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m236.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.21.1 (from requests->openmim)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m318.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting crcmod>=1.7 (from oss2~=2.17.0->openxlab->opendatalab->openmim)\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m249.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting aliyun-python-sdk-kms>=2.4.1 (from oss2~=2.17.0->openxlab->opendatalab->openmim)\n",
            "  Downloading aliyun_python_sdk_kms-2.16.3-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.1/98.1 kB\u001b[0m \u001b[31m252.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aliyun-python-sdk-core>=2.13.12 (from oss2~=2.17.0->openxlab->opendatalab->openmim)\n",
            "  Downloading aliyun-python-sdk-core-2.15.1.tar.gz (443 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.1/443.1 kB\u001b[0m \u001b[31m339.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jmespath<1.0.0,>=0.9.3 (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim)\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: cryptography>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (42.0.8)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (2.22)\n",
            "Building wheels for collected packages: oss2, aliyun-python-sdk-core, crcmod\n",
            "  Building wheel for oss2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for oss2: filename=oss2-2.17.0-py3-none-any.whl size=112372 sha256=62933e9de50cd3cdf46328e46320c306c3d98911a1abffed41a96e82f34cb27e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9x26vn_f/wheels/87/04/7b/7e61b8157fdf211c5131375240d0d86ca82e2a88ead9672c88\n",
            "  Building wheel for aliyun-python-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for aliyun-python-sdk-core: filename=aliyun_python_sdk_core-2.15.1-py3-none-any.whl size=535325 sha256=1472ec2fb7f71acc4f62af37adfab2f093b28cda85d0d7a7c356b2458e146fcd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9x26vn_f/wheels/69/4b/8e/0a28e00f4cf43b273c18cce083804738d41013e017da922ce4\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=31407 sha256=89ed19ea6f56af18127a446dee38a79cbdd9631c8dc94956bd8f403bffb3254f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9x26vn_f/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n",
            "Successfully built oss2 aliyun-python-sdk-core crcmod\n",
            "Installing collected packages: crcmod, urllib3, tqdm, setuptools, pycryptodome, ordered-set, jmespath, rich, requests, model-index, aliyun-python-sdk-core, aliyun-python-sdk-kms, oss2, openxlab, opendatalab, openmim\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.4\n",
            "    Uninstalling tqdm-4.66.4:\n",
            "      Successfully uninstalled tqdm-4.66.4\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.7.1\n",
            "    Uninstalling rich-13.7.1:\n",
            "      Successfully uninstalled rich-13.7.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "cvxpy 1.3.4 requires setuptools>65.5.1, but you have setuptools 60.2.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.28.2 which is incompatible.\n",
            "gradio 4.36.1 requires urllib3~=2.0, but you have urllib3 1.26.18 which is incompatible.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.12.0 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.0.1+cu118 which is incompatible.\n",
            "yfinance 0.2.40 requires requests>=2.31, but you have requests 2.28.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aliyun-python-sdk-core-2.15.1 aliyun-python-sdk-kms-2.16.3 crcmod-1.7 jmespath-0.10.0 model-index-0.1.11 opendatalab-0.0.10 openmim-0.3.9 openxlab-0.1.0 ordered-set-4.1.0 oss2-2.17.0 pycryptodome-3.20.0 requests-2.28.2 rich-13.4.2 setuptools-60.2.0 tqdm-4.65.2 urllib3-1.26.18\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "setuptools"
                ]
              },
              "id": "210ebe584a2d4a2385a71d3e4eced838"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/index.html\n",
            "Collecting mmengine\n",
            "  Downloading mmengine-0.10.4-py3-none-any.whl (451 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m451.7/451.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting addict (from mmengine)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mmengine) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mmengine) (1.23.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from mmengine) (6.0.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from mmengine) (13.4.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from mmengine) (2.4.0)\n",
            "Collecting yapf (from mmengine)\n",
            "  Downloading yapf-0.40.2-py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.7/254.7 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.10/dist-packages (from mmengine) (4.9.0.80)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (2.8.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmengine) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmengine) (2.16.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->mmengine) (7.1.0)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmengine) (4.2.2)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmengine) (2.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->mmengine) (3.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->mmengine) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mmengine) (1.16.0)\n",
            "Installing collected packages: addict, yapf, mmengine\n",
            "Successfully installed addict-2.4.0 mmengine-0.10.4 yapf-0.40.2\n",
            "Looking in links: https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/index.html\n",
            "Collecting mmcv>=2.0.1\n",
            "  Downloading https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/mmcv-2.2.0-cp310-cp310-manylinux1_x86_64.whl (98.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from mmcv>=2.0.1) (2.4.0)\n",
            "Requirement already satisfied: mmengine>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from mmcv>=2.0.1) (0.10.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mmcv>=2.0.1) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mmcv>=2.0.1) (24.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from mmcv>=2.0.1) (9.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from mmcv>=2.0.1) (6.0.1)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.10/dist-packages (from mmcv>=2.0.1) (0.40.2)\n",
            "Requirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.10/dist-packages (from mmcv>=2.0.1) (4.9.0.80)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mmengine>=0.3.0->mmcv>=2.0.1) (3.7.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from mmengine>=0.3.0->mmcv>=2.0.1) (13.4.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from mmengine>=0.3.0->mmcv>=2.0.1) (2.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->mmcv>=2.0.1) (7.1.0)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmcv>=2.0.1) (4.2.2)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmcv>=2.0.1) (2.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->mmcv>=2.0.1) (3.19.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv>=2.0.1) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv>=2.0.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv>=2.0.1) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv>=2.0.1) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv>=2.0.1) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv>=2.0.1) (2.8.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmengine>=0.3.0->mmcv>=2.0.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmengine>=0.3.0->mmcv>=2.0.1) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->mmengine>=0.3.0->mmcv>=2.0.1) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mmengine>=0.3.0->mmcv>=2.0.1) (1.16.0)\n",
            "Installing collected packages: mmcv\n",
            "Successfully installed mmcv-2.2.0\n",
            "Looking in links: https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/index.html\n",
            "Collecting mmdet>=3.1.0\n",
            "  Downloading mmdet-3.3.0-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mmdet>=3.1.0) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mmdet>=3.1.0) (1.23.5)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from mmdet>=3.1.0) (2.0.7)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mmdet>=3.1.0) (1.11.4)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (from mmdet>=3.1.0) (2.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from mmdet>=3.1.0) (1.16.0)\n",
            "Collecting terminaltables (from mmdet>=3.1.0)\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mmdet>=3.1.0) (4.65.2)\n",
            "Collecting mmcv<2.2.0,>=2.0.0rc4 (from mmdet>=3.1.0)\n",
            "  Downloading https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/mmcv-2.1.0-cp310-cp310-manylinux1_x86_64.whl (98.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mmengine<1.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from mmdet>=3.1.0) (0.10.4)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from mmcv<2.2.0,>=2.0.0rc4->mmdet>=3.1.0) (2.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mmcv<2.2.0,>=2.0.0rc4->mmdet>=3.1.0) (24.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from mmcv<2.2.0,>=2.0.0rc4->mmdet>=3.1.0) (9.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from mmcv<2.2.0,>=2.0.0rc4->mmdet>=3.1.0) (6.0.1)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.10/dist-packages (from mmcv<2.2.0,>=2.0.0rc4->mmdet>=3.1.0) (0.40.2)\n",
            "Requirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.10/dist-packages (from mmcv<2.2.0,>=2.0.0rc4->mmdet>=3.1.0) (4.9.0.80)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from mmengine<1.0.0,>=0.7.1->mmdet>=3.1.0) (13.4.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from mmengine<1.0.0,>=0.7.1->mmdet>=3.1.0) (2.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmdet>=3.1.0) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmdet>=3.1.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmdet>=3.1.0) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmdet>=3.1.0) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmdet>=3.1.0) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmdet>=3.1.0) (2.8.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmengine<1.0.0,>=0.7.1->mmdet>=3.1.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmengine<1.0.0,>=0.7.1->mmdet>=3.1.0) (2.16.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->mmcv<2.2.0,>=2.0.0rc4->mmdet>=3.1.0) (7.1.0)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmcv<2.2.0,>=2.0.0rc4->mmdet>=3.1.0) (4.2.2)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmcv<2.2.0,>=2.0.0rc4->mmdet>=3.1.0) (2.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->mmcv<2.2.0,>=2.0.0rc4->mmdet>=3.1.0) (3.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->mmengine<1.0.0,>=0.7.1->mmdet>=3.1.0) (0.1.2)\n",
            "Installing collected packages: terminaltables, mmcv, mmdet\n",
            "  Attempting uninstall: mmcv\n",
            "    Found existing installation: mmcv 2.2.0\n",
            "    Uninstalling mmcv-2.2.0:\n",
            "      Successfully uninstalled mmcv-2.2.0\n",
            "Successfully installed mmcv-2.1.0 mmdet-3.3.0 terminaltables-3.1.10\n",
            "Looking in links: https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/index.html\n",
            "Collecting mmpose>=1.1.0\n",
            "  Downloading mmpose-1.3.1-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chumpy (from mmpose>=1.1.0)\n",
            "  Downloading chumpy-0.70.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting json-tricks (from mmpose>=1.1.0)\n",
            "  Downloading json_tricks-3.17.3-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mmpose>=1.1.0) (3.7.1)\n",
            "Collecting munkres (from mmpose>=1.1.0)\n",
            "  Downloading munkres-1.1.4-py2.py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mmpose>=1.1.0) (1.23.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from mmpose>=1.1.0) (4.9.0.80)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from mmpose>=1.1.0) (9.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mmpose>=1.1.0) (1.11.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from mmpose>=1.1.0) (0.15.2+cu118)\n",
            "Collecting xtcocotools>=1.12 (from mmpose>=1.1.0)\n",
            "  Downloading xtcocotools-1.14.3-cp310-cp310-manylinux1_x86_64.whl (436 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.0/436.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mmcv<2.2.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from mmpose>=1.1.0) (2.1.0)\n",
            "Collecting mmdet<3.3.0,>=3.0.0 (from mmpose>=1.1.0)\n",
            "  Downloading mmdet-3.2.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mmengine<1.0.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mmpose>=1.1.0) (0.10.4)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from mmcv<2.2.0,>=2.0.0->mmpose>=1.1.0) (2.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mmcv<2.2.0,>=2.0.0->mmpose>=1.1.0) (24.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from mmcv<2.2.0,>=2.0.0->mmpose>=1.1.0) (6.0.1)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.10/dist-packages (from mmcv<2.2.0,>=2.0.0->mmpose>=1.1.0) (0.40.2)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from mmdet<3.3.0,>=3.0.0->mmpose>=1.1.0) (2.0.7)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (from mmdet<3.3.0,>=3.0.0->mmpose>=1.1.0) (2.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from mmdet<3.3.0,>=3.0.0->mmpose>=1.1.0) (1.16.0)\n",
            "Requirement already satisfied: terminaltables in /usr/local/lib/python3.10/dist-packages (from mmdet<3.3.0,>=3.0.0->mmpose>=1.1.0) (3.1.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mmdet<3.3.0,>=3.0.0->mmpose>=1.1.0) (4.65.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from mmengine<1.0.0,>=0.4.0->mmpose>=1.1.0) (13.4.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from mmengine<1.0.0,>=0.4.0->mmpose>=1.1.0) (2.4.0)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.10/dist-packages (from xtcocotools>=1.12->mmpose>=1.1.0) (60.2.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.10/dist-packages (from xtcocotools>=1.12->mmpose>=1.1.0) (3.0.10)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmpose>=1.1.0) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmpose>=1.1.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmpose>=1.1.0) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmpose>=1.1.0) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmpose>=1.1.0) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmpose>=1.1.0) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->mmpose>=1.1.0) (2.28.2)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchvision->mmpose>=1.1.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->mmpose>=1.1.0) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->mmpose>=1.1.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->mmpose>=1.1.0) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->mmpose>=1.1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->mmpose>=1.1.0) (3.1.4)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->mmpose>=1.1.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision->mmpose>=1.1.0) (3.27.9)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision->mmpose>=1.1.0) (18.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->mmpose>=1.1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->mmpose>=1.1.0) (3.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->mmpose>=1.1.0) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->mmpose>=1.1.0) (2024.6.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmengine<1.0.0,>=0.4.0->mmpose>=1.1.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmengine<1.0.0,>=0.4.0->mmpose>=1.1.0) (2.16.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->mmcv<2.2.0,>=2.0.0->mmpose>=1.1.0) (7.1.0)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmcv<2.2.0,>=2.0.0->mmpose>=1.1.0) (4.2.2)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmcv<2.2.0,>=2.0.0->mmpose>=1.1.0) (2.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->mmcv<2.2.0,>=2.0.0->mmpose>=1.1.0) (3.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->mmengine<1.0.0,>=0.4.0->mmpose>=1.1.0) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchvision->mmpose>=1.1.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchvision->mmpose>=1.1.0) (1.3.0)\n",
            "Building wheels for collected packages: chumpy\n",
            "  Building wheel for chumpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chumpy: filename=chumpy-0.70-py3-none-any.whl size=58281 sha256=65484898c29dcb0149e7c5b6aaa37dac8483da7851de33b76503edb22e1ba5ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/c1/ef/29ba7be03653a29ef6f2c3e1956d6c4d8877f2b243af411db1\n",
            "Successfully built chumpy\n",
            "Installing collected packages: munkres, json-tricks, chumpy, xtcocotools, mmdet, mmpose\n",
            "  Attempting uninstall: mmdet\n",
            "    Found existing installation: mmdet 3.3.0\n",
            "    Uninstalling mmdet-3.3.0:\n",
            "      Successfully uninstalled mmdet-3.3.0\n",
            "Successfully installed chumpy-0.70 json-tricks-3.17.3 mmdet-3.2.0 mmpose-1.3.1 munkres-1.1.4 xtcocotools-1.14.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Human_TalkingHead"
      ],
      "metadata": {
        "id": "QUgSSfnKjLdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dru3Xd7VUay",
        "outputId": "1b7b7526-f16d-41f5-f5b0-73d8fd9c4699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['FFMPEG_PATH'] = '/usr/bin/ffmpeg'"
      ],
      "metadata": {
        "id": "y54eWeh6Vjni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir -p /content/drive/MyDrive/MuseTalk\n",
        "# !git clone https://huggingface.co/TMElyralab/MuseTalk /content/drive/MyDrive/MuseTalk"
      ],
      "metadata": {
        "id": "mK1yIyFVVteJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ln -s /content/drive/MyDrive/MuseTalk/models /content/Human_TalkingHead/MuseTalk/models"
      ],
      "metadata": {
        "id": "KzO-iCzHWwEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import av"
      ],
      "metadata": {
        "id": "pK9HC9i2mXPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd StyleTTS2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHbIi0aUc85l",
        "outputId": "b2369100-4120-413a-91ed-92de56d74d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Human_TalkingHead/StyleTTS2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_content = \"\"\"\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import yaml\n",
        "import torchaudio\n",
        "import librosa\n",
        "from munch import Munch\n",
        "from nltk.tokenize import word_tokenize\n",
        "import phonemizer\n",
        "import time\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Ensure current directory is the script's directory\n",
        "os.chdir(os.path.dirname(os.path.abspath(__file__)))\n",
        "\n",
        "# Load the LLM output and audio filename from the command line arguments\n",
        "llm_output_path = os.path.abspath(sys.argv[1])\n",
        "audio_filename = os.path.abspath(sys.argv[2])\n",
        "\n",
        "# Initialize random seeds for reproducibility\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "# Load the LLM output from the file\n",
        "with open(llm_output_path, \"r\") as f:\n",
        "    text = f.read().strip()\n",
        "\n",
        "# Load packages\n",
        "from models import *\n",
        "from utils import *\n",
        "from text_utils import TextCleaner\n",
        "from Modules.diffusion.sampler import DiffusionSampler, ADPM2Sampler, KarrasSchedule\n",
        "from Utils.PLBERT.util import load_plbert\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "textcleaner = TextCleaner()\n",
        "\n",
        "# Load phonemizer\n",
        "global_phonemizer = phonemizer.backend.EspeakBackend(language='en-us', preserve_punctuation=True, with_stress=True, words_mismatch='ignore')\n",
        "\n",
        "# Load configuration and models\n",
        "config_path = os.path.join(\"Models/LJSpeech/config.yml\")\n",
        "with open(config_path, \"r\") as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "ASR_config = config.get('ASR_config', False)\n",
        "ASR_path = config.get('ASR_path', False)\n",
        "text_aligner = load_ASR_models(ASR_path, ASR_config)\n",
        "\n",
        "F0_path = config.get('F0_path', False)\n",
        "pitch_extractor = load_F0_models(F0_path)\n",
        "\n",
        "BERT_path = config.get('PLBERT_dir', False)\n",
        "plbert = load_plbert(BERT_path)\n",
        "\n",
        "model = build_model(recursive_munch(config['model_params']), text_aligner, pitch_extractor, plbert)\n",
        "_ = [model[key].eval() for key in model]\n",
        "_ = [model[key].to(device) for key in model]\n",
        "\n",
        "params_whole = torch.load(\"Models/LJSpeech/epoch_2nd_00100.pth\", map_location='cpu')\n",
        "params = params_whole['net']\n",
        "\n",
        "for key in model:\n",
        "    if key in params:\n",
        "        print('%s loaded' % key)\n",
        "        try:\n",
        "            model[key].load_state_dict(params[key])\n",
        "        except:\n",
        "            from collections import OrderedDict\n",
        "            state_dict = params[key]\n",
        "            new_state_dict = OrderedDict()\n",
        "            for k, v in state_dict.items():\n",
        "                name = k[7:]  # remove `module.`\n",
        "                new_state_dict[name] = v\n",
        "            model[key].load_state_dict(new_state_dict, strict=False)\n",
        "_ = [model[key].eval() for key in model]\n",
        "\n",
        "sampler = DiffusionSampler(\n",
        "    model.diffusion.diffusion,\n",
        "    sampler=ADPM2Sampler(),\n",
        "    sigma_schedule=KarrasSchedule(sigma_min=0.0001, sigma_max=3.0, rho=9.0),\n",
        "    clamp=False\n",
        ")\n",
        "\n",
        "def length_to_mask(lengths):\n",
        "    mask = torch.arange(lengths.max()).unsqueeze(0).expand(lengths.shape[0], -1).type_as(lengths)\n",
        "    mask = torch.gt(mask + 1, lengths.unsqueeze(1))\n",
        "    return mask\n",
        "\n",
        "to_mel = torchaudio.transforms.MelSpectrogram(\n",
        "    n_mels=80, n_fft=2048, win_length=1200, hop_length=300)\n",
        "mean, std = -4, 4\n",
        "\n",
        "def preprocess(wave):\n",
        "    wave_tensor = torch.from_numpy(wave).float()\n",
        "    mel_tensor = to_mel(wave_tensor)\n",
        "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
        "    return mel_tensor\n",
        "\n",
        "def inference(text, noise, diffusion_steps=5, embedding_scale=1):\n",
        "    text = text.strip().replace('\"', '')\n",
        "    ps = global_phonemizer.phonemize([text])\n",
        "    ps = word_tokenize(ps[0])\n",
        "    ps = ' '.join(ps)\n",
        "\n",
        "    tokens = textcleaner(ps)\n",
        "    tokens.insert(0, 0)\n",
        "    tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_lengths = torch.LongTensor([tokens.shape[-1]]).to(tokens.device)\n",
        "        text_mask = length_to_mask(input_lengths).to(tokens.device)\n",
        "\n",
        "        t_en = model.text_encoder(tokens, input_lengths, text_mask)\n",
        "        bert_dur = model.bert(tokens, attention_mask=(~text_mask).int())\n",
        "        d_en = model.bert_encoder(bert_dur).transpose(-1, -2)\n",
        "\n",
        "        s_pred = sampler(noise, embedding=bert_dur[0].unsqueeze(0), num_steps=diffusion_steps, embedding_scale=embedding_scale).squeeze(0)\n",
        "\n",
        "        s = s_pred[:, 128:]\n",
        "        ref = s_pred[:, :128]\n",
        "\n",
        "        d = model.predictor.text_encoder(d_en, s, input_lengths, text_mask)\n",
        "\n",
        "        x, _ = model.predictor.lstm(d)\n",
        "        duration = model.predictor.duration_proj(x)\n",
        "        duration = torch.sigmoid(duration).sum(axis=-1)\n",
        "        pred_dur = torch.round(duration.squeeze()).clamp(min=1)\n",
        "\n",
        "        pred_dur[-1] += 5\n",
        "\n",
        "        pred_aln_trg = torch.zeros(input_lengths, int(pred_dur.sum().data))\n",
        "        c_frame = 0\n",
        "        for i in range(pred_aln_trg.size(0)):\n",
        "            pred_aln_trg[i, c_frame:c_frame + int(pred_dur[i].data)] = 1\n",
        "            c_frame += int(pred_dur[i].data)\n",
        "\n",
        "        en = (d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device))\n",
        "        F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
        "        out = model.decoder((t_en @ pred_aln_trg.unsqueeze(0).to(device)), F0_pred, N_pred, ref.squeeze().unsqueeze(0))\n",
        "\n",
        "    return out.squeeze().cpu().numpy()\n",
        "\n",
        "def save_audio(audio_data, file_path):\n",
        "    torchaudio.save(file_path, torch.tensor(audio_data).unsqueeze(0), 24000)\n",
        "\n",
        "# Run the inference\n",
        "noise = torch.randn(1, 1, 256).to(device)\n",
        "start = time.time()\n",
        "wav = inference(text, noise, diffusion_steps=10, embedding_scale=1)\n",
        "rtf = (time.time() - start) / (len(wav) / 24000)\n",
        "print(f\"RTF = {rtf:5f}\")\n",
        "save_audio(wav, audio_filename)\n",
        "print(f\"Audio saved to {audio_filename}\")\n",
        "\"\"\"\n",
        "\n",
        "with open(\"run_inference.py\", \"w\") as file:\n",
        "    file.write(file_content)\n",
        "print(\"run_inference.py created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYP7HPuECjTK",
        "outputId": "3785184c-7987-4b89-d510-cada1903006e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run_inference.py created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oivRlNmNdP5l",
        "outputId": "2d993554-037c-442d-b850-e576963b96f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Human_TalkingHead\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd LIA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQXObJKforf0",
        "outputId": "8d1d9197-335c-4ee5-d1e1-da49d70884da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Human_TalkingHead/LIA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_content = \"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from networks.generator import Generator\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import os\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def load_image(filename, size):\n",
        "    img = Image.open(filename).convert('RGB')\n",
        "    img = img.resize((size, size), Image.LANCZOS)  # Use high-quality downsampling filter\n",
        "    img = np.asarray(img)\n",
        "    img = np.transpose(img, (2, 0, 1))  # Convert to channel first format\n",
        "\n",
        "    return img / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "\n",
        "\n",
        "def img_preprocessing(img_path, size):\n",
        "    img = load_image(img_path, size)  # [0, 1]\n",
        "    img = torch.from_numpy(img).unsqueeze(0).float()  # [0, 1]\n",
        "    imgs_norm = (img - 0.5) * 2.0  # [-1, 1]\n",
        "\n",
        "    return imgs_norm\n",
        "\n",
        "\n",
        "def vid_preprocessing(vid_path, size):\n",
        "    vid_dict = torchvision.io.read_video(vid_path, pts_unit='sec')\n",
        "    # Resize each frame in the video\n",
        "    frames = [frame.permute(2, 0, 1).unsqueeze(0) for frame in vid_dict[0]]\n",
        "    resized_frames = [torch.nn.functional.interpolate(frame.float(), size=(size, size), mode='bilinear', align_corners=False) for frame in frames]\n",
        "    vid = torch.cat(resized_frames, dim=0).unsqueeze(0)  # Reassemble video tensor\n",
        "    fps = vid_dict[2]['video_fps']\n",
        "    vid_norm = (vid / 255.0 - 0.5) * 2.0  # Normalize to [-1, 1]\n",
        "\n",
        "    return vid_norm, fps\n",
        "\n",
        "\n",
        "def save_video(vid_target_recon, save_path, fps):\n",
        "    vid = vid_target_recon.permute(0, 2, 3, 4, 1)\n",
        "    vid = vid.clamp(-1, 1).cpu()\n",
        "    vid = ((vid - vid.min()) / (vid.max() - vid.min()) * 255).type('torch.ByteTensor')\n",
        "\n",
        "    torchvision.io.write_video(save_path, vid[0], fps=fps)\n",
        "\n",
        "\n",
        "class Demo(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Demo, self).__init__()\n",
        "\n",
        "        self.args = args\n",
        "\n",
        "        if args.model == 'vox':\n",
        "            model_path = '/content/drive/MyDrive/ModelCheckpoints/vox.pt'\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        print('==> loading model')\n",
        "        self.gen = Generator(args.size, args.latent_dim_style, args.latent_dim_motion, args.channel_multiplier).cuda()\n",
        "        weight = torch.load(model_path, map_location=lambda storage, loc: storage)['gen']\n",
        "        self.gen.load_state_dict(weight)\n",
        "        self.gen.eval()\n",
        "\n",
        "        print('==> loading data')\n",
        "        self.save_path = args.save_folder + '/%s' % args.model\n",
        "        os.makedirs(self.save_path, exist_ok=True)\n",
        "        self.save_path = os.path.join(self.save_path, Path(args.source_path).stem + '_' + Path(args.driving_path).stem + '.mp4')\n",
        "        self.img_source = img_preprocessing(args.source_path, args.size).cuda()  # Make sure args.size is 256 or whatever your model expects\n",
        "        self.vid_target, self.fps = vid_preprocessing(args.driving_path, args.size)  # Same here\n",
        "        self.vid_target = self.vid_target.cuda()\n",
        "\n",
        "    def run(self):\n",
        "\n",
        "        print('==> running')\n",
        "        with torch.no_grad():\n",
        "\n",
        "            vid_target_recon = []\n",
        "\n",
        "            if self.args.model == 'ted':\n",
        "                h_start = None\n",
        "            else:\n",
        "                h_start = self.gen.enc.enc_motion(self.vid_target[:, 0, :, :, :])\n",
        "                print(\"Shape of h_start: \", h_start.shape)\n",
        "\n",
        "            for i in tqdm(range(self.vid_target.size(1))):\n",
        "                img_target = self.vid_target[:, i, :, :, :]\n",
        "                print(\"Shape of img_target at index {}: \".format(i), img_target.shape)\n",
        "                img_recon = self.gen(self.img_source, img_target, h_start)\n",
        "                print(\"Shape of img_recon: \", img_recon.shape)\n",
        "                vid_target_recon.append(img_recon.unsqueeze(2))\n",
        "\n",
        "            vid_target_recon = torch.cat(vid_target_recon, dim=2)\n",
        "            save_video(vid_target_recon, self.save_path, self.fps)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # training params\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--size\", type=int, default=256)\n",
        "    parser.add_argument(\"--channel_multiplier\", type=int, default=1)\n",
        "    parser.add_argument(\"--model\", type=str, choices=['vox', 'taichi', 'ted'], default='')\n",
        "    parser.add_argument(\"--latent_dim_style\", type=int, default=512)\n",
        "    parser.add_argument(\"--latent_dim_motion\", type=int, default=20)\n",
        "    parser.add_argument(\"--source_path\", type=str, default='')\n",
        "    parser.add_argument(\"--driving_path\", type=str, default='')\n",
        "    parser.add_argument(\"--save_folder\", type=str, default='./res')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # demo\n",
        "    demo = Demo(args)\n",
        "    demo.run()\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "with open(\"run_demo.py\", \"w\") as file:\n",
        "    file.write(file_content)\n",
        "print(\"run_demo.py created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Wqqw2Wroag7",
        "outputId": "e5876f2b-10ad-4996-d4f3-ee93b1642512"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run_demo.py created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkP4KnMJowsj",
        "outputId": "8da9d24a-360c-480a-bce2-46aa9fdb17e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Human_TalkingHead\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_content = \"\"\"\n",
        "import argparse\n",
        "import os\n",
        "from omegaconf import OmegaConf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import glob\n",
        "import pickle\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "import json\n",
        "from musetalk.utils.utils import get_file_type,get_video_fps,datagen\n",
        "from musetalk.utils.preprocessing import get_landmark_and_bbox,read_imgs,coord_placeholder\n",
        "from musetalk.utils.blending import get_image,get_image_prepare_material,get_image_blending\n",
        "from musetalk.utils.utils import load_all_model\n",
        "import shutil\n",
        "\n",
        "import threading\n",
        "import queue\n",
        "\n",
        "import time\n",
        "\n",
        "# load model weights\n",
        "audio_processor, vae, unet, pe = load_all_model()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "timesteps = torch.tensor([0], device=device)\n",
        "pe = pe.half()\n",
        "vae.vae = vae.vae.half()\n",
        "unet.model = unet.model.half()\n",
        "\n",
        "def video2imgs(vid_path, save_path, ext = '.png',cut_frame = 10000000):\n",
        "    cap = cv2.VideoCapture(vid_path)\n",
        "    count = 0\n",
        "    while True:\n",
        "        if count > cut_frame:\n",
        "            break\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            cv2.imwrite(f\"{save_path}/{count:08d}.png\", frame)\n",
        "            count += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "def osmakedirs(path_list):\n",
        "    for path in path_list:\n",
        "        os.makedirs(path) if not os.path.exists(path) else None\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "class Avatar:\n",
        "    def __init__(self, avatar_id, video_path, bbox_shift, batch_size, preparation):\n",
        "        self.avatar_id = avatar_id\n",
        "        self.video_path = video_path\n",
        "        self.bbox_shift = bbox_shift\n",
        "        self.avatar_path = f\"./results/avatars/{avatar_id}\"\n",
        "        self.full_imgs_path = f\"{self.avatar_path}/full_imgs\"\n",
        "        self.coords_path = f\"{self.avatar_path}/coords.pkl\"\n",
        "        self.latents_out_path= f\"{self.avatar_path}/latents.pt\"\n",
        "        self.video_out_path = f\"{self.avatar_path}/vid_output/\"\n",
        "        self.mask_out_path =f\"{self.avatar_path}/mask\"\n",
        "        self.mask_coords_path =f\"{self.avatar_path}/mask_coords.pkl\"\n",
        "        self.avatar_info_path = f\"{self.avatar_path}/avator_info.json\"\n",
        "        self.avatar_info = {\n",
        "            \"avatar_id\":avatar_id,\n",
        "            \"video_path\":video_path,\n",
        "            \"bbox_shift\":bbox_shift\n",
        "        }\n",
        "        self.preparation = preparation\n",
        "        self.batch_size = batch_size\n",
        "        self.idx = 0\n",
        "        self.init()\n",
        "\n",
        "    def init(self):\n",
        "        if self.preparation:\n",
        "            if os.path.exists(self.avatar_path):\n",
        "                # If the avatar already exists, do not recreate it, just load the existing data\n",
        "                print(f\"{self.avatar_id} already exists, proceeding with the existing avatar.\")\n",
        "                self.input_latent_list_cycle = torch.load(self.latents_out_path)\n",
        "                with open(self.coords_path, 'rb') as f:\n",
        "                    self.coord_list_cycle = pickle.load(f)\n",
        "                input_img_list = glob.glob(os.path.join(self.full_imgs_path, '*.[jpJP][pnPN]*[gG]'))\n",
        "                input_img_list = sorted(input_img_list, key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))\n",
        "                self.frame_list_cycle = read_imgs(input_img_list)\n",
        "                with open(self.mask_coords_path, 'rb') as f:\n",
        "                    self.mask_coords_list_cycle = pickle.load(f)\n",
        "                input_mask_list = glob.glob(os.path.join(self.mask_out_path, '*.[jpJP][pnPN]*[gG]'))\n",
        "                input_mask_list = sorted(input_mask_list, key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))\n",
        "                self.mask_list_cycle = read_imgs(input_mask_list)\n",
        "            else:\n",
        "                print(\"*********************************\")\n",
        "                print(f\"  creating avator: {self.avatar_id}\")\n",
        "                print(\"*********************************\")\n",
        "                osmakedirs([self.avatar_path,self.full_imgs_path,self.video_out_path,self.mask_out_path])\n",
        "                self.prepare_material()\n",
        "        else:\n",
        "            if not os.path.exists(self.avatar_path):\n",
        "                print(f\"{self.avatar_id} does not exist, you should set preparation to True\")\n",
        "                sys.exit()\n",
        "\n",
        "            with open(self.avatar_info_path, \"r\") as f:\n",
        "                avatar_info = json.load(f)\n",
        "\n",
        "            if avatar_info['bbox_shift'] != self.avatar_info['bbox_shift']:\n",
        "                response = input(f\" 【bbox_shift】 is changed, you need to re-create it ! (c/continue)\")\n",
        "                if response.lower() == \"c\":\n",
        "                    shutil.rmtree(self.avatar_path)\n",
        "                    print(\"*********************************\")\n",
        "                    print(f\"  creating avator: {self.avatar_id}\")\n",
        "                    print(\"*********************************\")\n",
        "                    osmakedirs([self.avatar_path,self.full_imgs_path,self.video_out_path,self.mask_out_path])\n",
        "                    self.prepare_material()\n",
        "                else:\n",
        "                    sys.exit()\n",
        "            else:\n",
        "                self.input_latent_list_cycle = torch.load(self.latents_out_path)\n",
        "                with open(self.coords_path, 'rb') as f:\n",
        "                    self.coord_list_cycle = pickle.load(f)\n",
        "                input_img_list = glob.glob(os.path.join(self.full_imgs_path, '*.[jpJP][pnPN]*[gG]'))\n",
        "                input_img_list = sorted(input_img_list, key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))\n",
        "                self.frame_list_cycle = read_imgs(input_img_list)\n",
        "                with open(self.mask_coords_path, 'rb') as f:\n",
        "                    self.mask_coords_list_cycle = pickle.load(f)\n",
        "                input_mask_list = glob.glob(os.path.join(self.mask_out_path, '*.[jpJP][pnPN]*[gG]'))\n",
        "                input_mask_list = sorted(input_mask_list, key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))\n",
        "                self.mask_list_cycle = read_imgs(input_mask_list)\n",
        "\n",
        "\n",
        "    def prepare_material(self):\n",
        "        print(\"preparing data materials ... ...\")\n",
        "        with open(self.avatar_info_path, \"w\") as f:\n",
        "            json.dump(self.avatar_info, f)\n",
        "\n",
        "        if os.path.isfile(self.video_path):\n",
        "            video2imgs(self.video_path, self.full_imgs_path, ext = 'png')\n",
        "        else:\n",
        "            print(f\"copy files in {self.video_path}\")\n",
        "            files = os.listdir(self.video_path)\n",
        "            files.sort()\n",
        "            files = [file for file in files if file.split(\".\")[-1]==\"png\"]\n",
        "            for filename in files:\n",
        "                shutil.copyfile(f\"{self.video_path}/{filename}\", f\"{self.full_imgs_path}/{filename}\")\n",
        "        input_img_list = sorted(glob.glob(os.path.join(self.full_imgs_path, '*.[jpJP][pnPN]*[gG]')))\n",
        "\n",
        "        print(\"extracting landmarks...\")\n",
        "        coord_list, frame_list = get_landmark_and_bbox(input_img_list, self.bbox_shift)\n",
        "        input_latent_list = []\n",
        "        idx = -1\n",
        "        # maker if the bbox is not sufficient\n",
        "        coord_placeholder = (0.0,0.0,0.0,0.0)\n",
        "        for bbox, frame in zip(coord_list, frame_list):\n",
        "            idx = idx + 1\n",
        "            if bbox == coord_placeholder:\n",
        "                continue\n",
        "            x1, y1, x2, y2 = bbox\n",
        "            crop_frame = frame[y1:y2, x1:x2]\n",
        "            resized_crop_frame = cv2.resize(crop_frame,(256,256),interpolation = cv2.INTER_LANCZOS4)\n",
        "            latents = vae.get_latents_for_unet(resized_crop_frame)\n",
        "            input_latent_list.append(latents)\n",
        "\n",
        "        self.frame_list_cycle = frame_list + frame_list[::-1]\n",
        "        self.coord_list_cycle = coord_list + coord_list[::-1]\n",
        "        self.input_latent_list_cycle = input_latent_list + input_latent_list[::-1]\n",
        "        self.mask_coords_list_cycle = []\n",
        "        self.mask_list_cycle = []\n",
        "\n",
        "        for i,frame in enumerate(tqdm(self.frame_list_cycle)):\n",
        "            cv2.imwrite(f\"{self.full_imgs_path}/{str(i).zfill(8)}.png\",frame)\n",
        "\n",
        "            face_box = self.coord_list_cycle[i]\n",
        "            mask,crop_box = get_image_prepare_material(frame,face_box)\n",
        "            cv2.imwrite(f\"{self.mask_out_path}/{str(i).zfill(8)}.png\",mask)\n",
        "            self.mask_coords_list_cycle += [crop_box]\n",
        "            self.mask_list_cycle.append(mask)\n",
        "\n",
        "        with open(self.mask_coords_path, 'wb') as f:\n",
        "            pickle.dump(self.mask_coords_list_cycle, f)\n",
        "\n",
        "        with open(self.coords_path, 'wb') as f:\n",
        "            pickle.dump(self.coord_list_cycle, f)\n",
        "\n",
        "        torch.save(self.input_latent_list_cycle, os.path.join(self.latents_out_path))\n",
        "        #\n",
        "\n",
        "    def process_frames(self,\n",
        "                       res_frame_queue,\n",
        "                       video_len,\n",
        "                       skip_save_images):\n",
        "        print(video_len)\n",
        "        while True:\n",
        "            if self.idx>=video_len-1:\n",
        "                break\n",
        "            try:\n",
        "                start = time.time()\n",
        "                res_frame = res_frame_queue.get(block=True, timeout=1)\n",
        "            except queue.Empty:\n",
        "                continue\n",
        "\n",
        "            bbox = self.coord_list_cycle[self.idx%(len(self.coord_list_cycle))]\n",
        "            ori_frame = copy.deepcopy(self.frame_list_cycle[self.idx%(len(self.frame_list_cycle))])\n",
        "            x1, y1, x2, y2 = bbox\n",
        "            try:\n",
        "                res_frame = cv2.resize(res_frame.astype(np.uint8),(x2-x1,y2-y1))\n",
        "            except:\n",
        "                continue\n",
        "            mask = self.mask_list_cycle[self.idx%(len(self.mask_list_cycle))]\n",
        "            mask_crop_box = self.mask_coords_list_cycle[self.idx%(len(self.mask_coords_list_cycle))]\n",
        "            #combine_frame = get_image(ori_frame,res_frame,bbox)\n",
        "            combine_frame = get_image_blending(ori_frame,res_frame,bbox,mask,mask_crop_box)\n",
        "\n",
        "            if skip_save_images is False:\n",
        "                cv2.imwrite(f\"{self.avatar_path}/tmp/{str(self.idx).zfill(8)}.png\",combine_frame)\n",
        "            self.idx = self.idx + 1\n",
        "\n",
        "    def inference(self,\n",
        "                  audio_path,\n",
        "                  out_vid_name,\n",
        "                  fps,\n",
        "                  skip_save_images):\n",
        "        os.makedirs(self.avatar_path+'/tmp',exist_ok =True)\n",
        "        print(\"start inference\")\n",
        "        ############################################## extract audio feature ##############################################\n",
        "        start_time = time.time()\n",
        "        whisper_feature = audio_processor.audio2feat(audio_path)\n",
        "        whisper_chunks = audio_processor.feature2chunks(feature_array=whisper_feature,fps=fps)\n",
        "        print(f\"processing audio:{audio_path} costs {(time.time() - start_time) * 1000}ms\")\n",
        "        ############################################## inference batch by batch ##############################################\n",
        "        video_num = len(whisper_chunks)\n",
        "        res_frame_queue = queue.Queue()\n",
        "        self.idx = 0\n",
        "        # # Create a sub-thread and start it\n",
        "        process_thread = threading.Thread(target=self.process_frames, args=(res_frame_queue, video_num, skip_save_images))\n",
        "        process_thread.start()\n",
        "\n",
        "        gen = datagen(whisper_chunks,\n",
        "                      self.input_latent_list_cycle,\n",
        "                      self.batch_size)\n",
        "        start_time = time.time()\n",
        "        res_frame_list = []\n",
        "\n",
        "        for i, (whisper_batch,latent_batch) in enumerate(tqdm(gen,total=int(np.ceil(float(video_num)/self.batch_size)))):\n",
        "            audio_feature_batch = torch.from_numpy(whisper_batch)\n",
        "            audio_feature_batch = audio_feature_batch.to(device=unet.device,\n",
        "                                                         dtype=unet.model.dtype)\n",
        "            audio_feature_batch = pe(audio_feature_batch)\n",
        "            latent_batch = latent_batch.to(dtype=unet.model.dtype)\n",
        "\n",
        "            pred_latents = unet.model(latent_batch,\n",
        "                                      timesteps,\n",
        "                                      encoder_hidden_states=audio_feature_batch).sample\n",
        "            recon = vae.decode_latents(pred_latents)\n",
        "            for res_frame in recon:\n",
        "                res_frame_queue.put(res_frame)\n",
        "        # Close the queue and sub-thread after all tasks are completed\n",
        "        process_thread.join()\n",
        "\n",
        "        if args.skip_save_images is True:\n",
        "            print('Total process time of {} frames without saving images = {}s'.format(\n",
        "                        video_num,\n",
        "                        time.time()-start_time))\n",
        "        else:\n",
        "            print('Total process time of {} frames including saving images = {}s'.format(\n",
        "                        video_num,\n",
        "                        time.time()-start_time))\n",
        "\n",
        "        if out_vid_name is not None and args.skip_save_images is False:\n",
        "            # optional\n",
        "            cmd_img2video = f\"ffmpeg -y -v warning -r {fps} -f image2 -i {self.avatar_path}/tmp/%08d.png -vcodec libx264 -vf format=rgb24,scale=out_color_matrix=bt709,format=yuv420p -crf 18 {self.avatar_path}/temp.mp4\"\n",
        "            print(cmd_img2video)\n",
        "            os.system(cmd_img2video)\n",
        "\n",
        "            output_vid = os.path.join(self.video_out_path, out_vid_name+\".mp4\") # on\n",
        "            cmd_combine_audio = f\"ffmpeg -y -v warning -i {audio_path} -i {self.avatar_path}/temp.mp4 {output_vid}\"\n",
        "            print(cmd_combine_audio)\n",
        "            os.system(cmd_combine_audio)\n",
        "\n",
        "            os.remove(f\"{self.avatar_path}/temp.mp4\")\n",
        "            shutil.rmtree(f\"{self.avatar_path}/tmp\")\n",
        "            print(f\"result is save to {output_vid}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    '''\n",
        "    This script is used to simulate online chatting and applies necessary pre-processing such as face detection and face parsing in advance. During online chatting, only UNet and the VAE decoder are involved, which makes MuseTalk real-time.\n",
        "    '''\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--inference_config\",\n",
        "                        type=str,\n",
        "                        default=\"configs/inference/realtime.yaml\",\n",
        "    )\n",
        "    parser.add_argument(\"--fps\",\n",
        "                        type=int,\n",
        "                        default=25,\n",
        "    )\n",
        "    parser.add_argument(\"--batch_size\",\n",
        "                        type=int,\n",
        "                        default=4,\n",
        "    )\n",
        "    parser.add_argument(\"--skip_save_images\",\n",
        "                        action=\"store_true\",\n",
        "                        help=\"Whether skip saving images for better generation speed calculation\",\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    inference_config = OmegaConf.load(args.inference_config)\n",
        "    print(inference_config)\n",
        "\n",
        "\n",
        "    for avatar_id in inference_config:\n",
        "        data_preparation = inference_config[avatar_id][\"preparation\"]\n",
        "        video_path = inference_config[avatar_id][\"video_path\"]\n",
        "        bbox_shift = inference_config[avatar_id][\"bbox_shift\"]\n",
        "        avatar = Avatar(\n",
        "            avatar_id = avatar_id,\n",
        "            video_path = video_path,\n",
        "            bbox_shift = bbox_shift,\n",
        "            batch_size = args.batch_size,\n",
        "            preparation= data_preparation)\n",
        "\n",
        "        audio_clips = inference_config[avatar_id][\"audio_clips\"]\n",
        "        for audio_num, audio_path in audio_clips.items():\n",
        "            print(\"Inferring using:\",audio_path)\n",
        "            avatar.inference(audio_path,\n",
        "                             audio_num,\n",
        "                             args.fps,\n",
        "                             args.skip_save_images)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "with open(\"MuseTalk/scripts/realtime_inference.py\", \"w\") as file:\n",
        "    file.write(file_content)\n",
        "print(\"/realtime_inference.py created successfully!\")\n"
      ],
      "metadata": {
        "id": "IskCIbCHlSZd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab0fb7ac-fc66-415e-b66c-80c2f8e7eb04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/realtime_inference.py created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok --quiet"
      ],
      "metadata": {
        "id": "V6YunafT7ZPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.kill()\n",
        "\n",
        "auth_token = \"2hsgXUeGzAJoAMH4R1cOWSuzWwP_2UAzc8eteNjuGioKV9tKU\""
      ],
      "metadata": {
        "id": "rfD9enJR7HRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.set_auth_token(auth_token)"
      ],
      "metadata": {
        "id": "mZfPfBA37WMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok_tunnel = ngrok.connect(7000)"
      ],
      "metadata": {
        "id": "FGK9ixT_7dCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok_url = ngrok_tunnel.public_url"
      ],
      "metadata": {
        "id": "NugE9c_2h8Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"ngrok URL: {ngrok_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1bNJqiViD3M",
        "outputId": "078f556a-8d96-4fcf-d479-3b2fbc664b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok URL: https://79e0-35-240-242-58.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "settings_file_path = \"virtual_therapist/settings.py\"\n",
        "with open(settings_file_path, \"r\") as file:\n",
        "    settings = file.read()\n",
        "\n",
        "allowed_hosts_pattern = re.compile(r\"ALLOWED_HOSTS = \\[.*?\\]\", re.DOTALL)\n",
        "csrf_trusted_origins_pattern = re.compile(r\"CSRF_TRUSTED_ORIGINS = \\[.*?\\]\", re.DOTALL)\n",
        "\n",
        "new_allowed_hosts = f\"ALLOWED_HOSTS = ['localhost', '127.0.0.1', '{ngrok_url.lstrip('https://').lstrip('http://')}']\"\n",
        "new_csrf_trusted_origins = f\"CSRF_TRUSTED_ORIGINS = ['{ngrok_url}']\"\n",
        "\n",
        "print(f\"New ALLOWED_HOSTS: {new_allowed_hosts}\")\n",
        "print(f\"New CSRF_TRUSTED_ORIGINS: {new_csrf_trusted_origins}\")\n",
        "\n",
        "settings = allowed_hosts_pattern.sub(new_allowed_hosts, settings)\n",
        "settings = csrf_trusted_origins_pattern.sub(new_csrf_trusted_origins, settings)\n",
        "\n",
        "with open(settings_file_path, \"w\") as file:\n",
        "    file.write(settings)\n",
        "\n",
        "with open(settings_file_path, \"r\") as file:\n",
        "    updated_settings = file.read()\n",
        "    print(f\"Updated settings.py content:\\n{updated_settings}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4VrBabNiU2W",
        "outputId": "b03c1dc8-b875-4f69-aecd-42e241df3072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New ALLOWED_HOSTS: ALLOWED_HOSTS = ['localhost', '127.0.0.1', '79e0-35-240-242-58.ngrok-free.app']\n",
            "New CSRF_TRUSTED_ORIGINS: CSRF_TRUSTED_ORIGINS = ['https://79e0-35-240-242-58.ngrok-free.app']\n",
            "Updated settings.py content:\n",
            "\"\"\"\n",
            "Django settings for virtual_therapist project.\n",
            "\n",
            "Generated by 'django-admin startproject' using Django 4.2.13.\n",
            "\n",
            "For more information on this file, see\n",
            "https://docs.djangoproject.com/en/4.2/topics/settings/\n",
            "\n",
            "For the full list of settings and their values, see\n",
            "https://docs.djangoproject.com/en/4.2/ref/settings/\n",
            "\"\"\"\n",
            "\n",
            "from pathlib import Path\n",
            "import os\n",
            "\n",
            "# Build paths inside the project like this: BASE_DIR / 'subdir'.\n",
            "BASE_DIR = Path(__file__).resolve().parent.parent\n",
            "\n",
            "\n",
            "# Quick-start development settings - unsuitable for production\n",
            "# See https://docs.djangoproject.com/en/4.2/howto/deployment/checklist/\n",
            "\n",
            "# SECURITY WARNING: keep the secret key used in production secret!\n",
            "SECRET_KEY = \"django-insecure-5i08*y*1s5e)eom*8+iy4^ly9s5h%9zvl*=o5n596acb$14p_8\"\n",
            "\n",
            "# SECURITY WARNING: don't run with debug turned on in production!\n",
            "DEBUG = True\n",
            "\n",
            "ALLOWED_HOSTS = ['localhost', '127.0.0.1', '79e0-35-240-242-58.ngrok-free.app']\n",
            "CSRF_TRUSTED_ORIGINS = ['https://79e0-35-240-242-58.ngrok-free.app']\n",
            "\n",
            "# Application definition\n",
            "\n",
            "INSTALLED_APPS = [\n",
            "    \"django.contrib.admin\",\n",
            "    \"django.contrib.auth\",\n",
            "    \"django.contrib.contenttypes\",\n",
            "    \"django.contrib.sessions\",\n",
            "    \"django.contrib.messages\",\n",
            "    \"django.contrib.staticfiles\",\n",
            "    \"therapy\",\n",
            "    \"rest_framework\",\n",
            "]\n",
            "\n",
            "MIDDLEWARE = [\n",
            "    \"django.middleware.security.SecurityMiddleware\",\n",
            "    \"django.contrib.sessions.middleware.SessionMiddleware\",\n",
            "    \"django.middleware.common.CommonMiddleware\",\n",
            "    \"django.middleware.csrf.CsrfViewMiddleware\",\n",
            "    \"django.contrib.auth.middleware.AuthenticationMiddleware\",\n",
            "    \"django.contrib.messages.middleware.MessageMiddleware\",\n",
            "    \"django.middleware.clickjacking.XFrameOptionsMiddleware\",\n",
            "]\n",
            "\n",
            "ROOT_URLCONF = \"virtual_therapist.urls\"\n",
            "\n",
            "TEMPLATES = [\n",
            "    {\n",
            "        \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n",
            "        \"DIRS\": [],\n",
            "        \"APP_DIRS\": True,\n",
            "        \"OPTIONS\": {\n",
            "            \"context_processors\": [\n",
            "                \"django.template.context_processors.debug\",\n",
            "                \"django.template.context_processors.request\",\n",
            "                \"django.contrib.auth.context_processors.auth\",\n",
            "                \"django.contrib.messages.context_processors.messages\",\n",
            "            ],\n",
            "        },\n",
            "    },\n",
            "]\n",
            "\n",
            "WSGI_APPLICATION = \"virtual_therapist.wsgi.application\"\n",
            "\n",
            "\n",
            "# Database\n",
            "# https://docs.djangoproject.com/en/4.2/ref/settings/#databases\n",
            "\n",
            "DATABASES = {\n",
            "    \"default\": {\n",
            "        \"ENGINE\": \"django.db.backends.sqlite3\",\n",
            "        \"NAME\": BASE_DIR / \"db.sqlite3\",\n",
            "    }\n",
            "}\n",
            "\n",
            "\n",
            "# Password validation\n",
            "# https://docs.djangoproject.com/en/4.2/ref/settings/#auth-password-validators\n",
            "\n",
            "AUTH_PASSWORD_VALIDATORS = [\n",
            "    {\n",
            "        \"NAME\": \"django.contrib.auth.password_validation.UserAttributeSimilarityValidator\",\n",
            "    },\n",
            "    {\n",
            "        \"NAME\": \"django.contrib.auth.password_validation.MinimumLengthValidator\",\n",
            "    },\n",
            "    {\n",
            "        \"NAME\": \"django.contrib.auth.password_validation.CommonPasswordValidator\",\n",
            "    },\n",
            "    {\n",
            "        \"NAME\": \"django.contrib.auth.password_validation.NumericPasswordValidator\",\n",
            "    },\n",
            "]\n",
            "\n",
            "\n",
            "# Internationalization\n",
            "# https://docs.djangoproject.com/en/4.2/topics/i18n/\n",
            "\n",
            "LANGUAGE_CODE = \"en-us\"\n",
            "\n",
            "TIME_ZONE = \"UTC\"\n",
            "\n",
            "USE_I18N = True\n",
            "\n",
            "USE_TZ = True\n",
            "\n",
            "\n",
            "# Static files (CSS, JavaScript, Images)\n",
            "# https://docs.djangoproject.com/en/4.2/howto/static-files/\n",
            "\n",
            "STATIC_URL = 'static/'\n",
            "\n",
            "\n",
            "\n",
            "# Default primary key field type\n",
            "# https://docs.djangoproject.com/en/4.2/ref/settings/#default-auto-field\n",
            "\n",
            "DEFAULT_AUTO_FIELD = \"django.db.models.BigAutoField\"\n",
            "\n",
            "MEDIA_URL = '/media/'\n",
            "MEDIA_ROOT = os.path.join(BASE_DIR, 'media')\n",
            "\n",
            "VIDEO_URL = '/videos/'\n",
            "VIDEO_ROOT = os.path.join(BASE_DIR, 'MuseTalk/results/avatars/avator_1/vid_output')\n",
            "\n",
            "MUSETALK_URL = '/MuseTalk/'\n",
            "MUSETALK_ROOT = os.path.join(BASE_DIR, 'MuseTalk')\n",
            "# Static files (CSS, JavaScript, Images)\n",
            "STATIC_URL = '/static/'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python manage.py makemigrations\n",
        "!python manage.py migrate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLxoJwck72Ir",
        "outputId": "bd0ecff1-858a-4058-de33-0f3ebe828f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-06-17 06:43:33.977376: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-17 06:43:36.581271: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "24-06-17 06:43:39 - Directory /root/.deepface created\n",
            "24-06-17 06:43:39 - Directory /root/.deepface/weights created\n",
            "\u001b[36;1mMigrations for 'therapy':\u001b[0m\n",
            "  \u001b[1mtherapy/migrations/0007_therapistresponse_audio_and_more.py\u001b[0m\n",
            "    - Add field audio to therapistresponse\n",
            "    - Alter field session_id on therapistresponse\n",
            "2024-06-17 06:43:45.377869: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-17 06:43:46.329505: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36;1mOperations to perform:\u001b[0m\n",
            "\u001b[1m  Apply all migrations: \u001b[0madmin, auth, contenttypes, sessions, therapy\n",
            "\u001b[36;1mRunning migrations:\u001b[0m\n",
            "  Applying therapy.0007_therapistresponse_audio_and_more...\u001b[32;1m OK\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python manage.py runserver 7000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0IiGUY87iv4",
        "outputId": "ca72fa00-fd2d-4098-a53d-977217be45a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Watching for file changes with StatReloader\n",
            "Performing system checks...\n",
            "\n",
            "2024-06-17 08:04:03.328627: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-17 08:04:05.975512: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "System check identified no issues (0 silenced).\n",
            "June 17, 2024 - 08:04:10\n",
            "Django version 5.0.6, using settings 'virtual_therapist.settings'\n",
            "Starting development server at http://127.0.0.1:7000/\n",
            "Quit the server with CONTROL-C.\n",
            "\n",
            "[17/Jun/2024 08:04:19] \u001b[m\"GET / HTTP/1.1\" 200 12767\u001b[0m\n",
            "Not Found: /favicon.ico\n",
            "[17/Jun/2024 08:04:20] \u001b[33m\"GET /favicon.ico HTTP/1.1\" 404 3099\u001b[0m\n",
            "Received timestamp: 0\n",
            "Analyzing frame...\n",
            "2024-06-17 08:04:24.344063: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-06-17 08:04:25.047152: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "Received timestamp: 1012\n",
            "Analyzing frame...\n",
            "Received timestamp: 2382\n",
            "Analyzing frame...\n",
            "Received timestamp: 4385\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.555985454081622, 'disgust': 0.0004510180402193494, 'fear': 5.6618917382891025, 'happy': 0.0076273075136334455, 'sad': 31.468667475501743, 'surprise': 0.0012466588320430975, 'neutral': 62.3041309299547}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:04:28] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Analysis result: [{'emotion': {'angry': 0.2743708901107311, 'disgust': 5.016672943725098e-06, 'fear': 2.74693313986063, 'happy': 0.0014921489309926983, 'sad': 41.179996728897095, 'surprise': 0.00048492256610188633, 'neutral': 55.79671859741211}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:04:28] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Analysis result: [{'emotion': {'angry': 0.0026559917387203313, 'disgust': 4.5332612663706584e-08, 'fear': 3.4920766949653625, 'happy': 3.990367929418426e-05, 'sad': 2.435656450688839, 'surprise': 1.5779486162159628e-06, 'neutral': 94.06957030296326}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:04:28] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Analysis result: [{'emotion': {'angry': 0.04622856197750938, 'disgust': 2.5311310113285816e-06, 'fear': 0.000307560590187032, 'happy': 97.45547160240838, 'sad': 0.1005489788345384, 'surprise': 7.299806705668863e-05, 'neutral': 2.397368804953623}, 'dominant_emotion': 'happy', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:04:29] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 6382\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 14.794926345348358, 'disgust': 0.0006119184490671614, 'fear': 5.224212259054184, 'happy': 0.576859712600708, 'sad': 1.603752188384533, 'surprise': 61.5526556968689, 'neutral': 16.24697744846344}, 'dominant_emotion': 'surprise', 'region': {'x': 35, 'y': 142, 'w': 63, 'h': 63, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.94}]\n",
            "[17/Jun/2024 08:04:30] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 22\u001b[0m\n",
            "==> loading model\n",
            "Received timestamp: 8383\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.9103748016059399, 'disgust': 4.816933074636154e-07, 'fear': 0.26216753758490086, 'happy': 0.0034136202884837985, 'sad': 1.9216401502490044, 'surprise': 0.00529049793840386, 'neutral': 96.89711332321167}, 'dominant_emotion': 'neutral', 'region': {'x': 227, 'y': 126, 'w': 204, 'h': 204, 'left_eye': (353, 198), 'right_eye': (289, 203)}, 'face_confidence': 0.94}]\n",
            "[17/Jun/2024 08:04:32] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 10384\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 58.0721189889766, 'disgust': 0.3271198674371997, 'fear': 7.44556695871747, 'happy': 0.021906134796808238, 'sad': 15.786602701353704, 'surprise': 0.23003664850751776, 'neutral': 18.116643355292872}, 'dominant_emotion': 'angry', 'region': {'x': 240, 'y': 139, 'w': 205, 'h': 205, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.95}]\n",
            "[17/Jun/2024 08:04:34] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "==> loading data\n",
            "Received timestamp: 12389\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 20.767601338910897, 'disgust': 0.004737376165451343, 'fear': 10.985179426052639, 'happy': 0.034427024627421844, 'sad': 24.239425451824015, 'surprise': 0.036311034673999584, 'neutral': 43.932316430530975}, 'dominant_emotion': 'neutral', 'region': {'x': 240, 'y': 132, 'w': 203, 'h': 203, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.95}]\n",
            "[17/Jun/2024 08:04:37] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 14402\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 14.3680140376091, 'disgust': 4.6135821207826666e-05, 'fear': 1.116512157022953, 'happy': 0.01149923336924985, 'sad': 6.356576830148697, 'surprise': 0.02579278952907771, 'neutral': 78.12156081199646}, 'dominant_emotion': 'neutral', 'region': {'x': 221, 'y': 134, 'w': 215, 'h': 215, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.95}]\n",
            "[17/Jun/2024 08:04:39] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "==> running\n",
            "Shape of h_start:  torch.Size([1, 20])\n",
            "  0% 0/425 [00:00<?, ?it/s]Shape of img_target at index 0:  torch.Size([1, 3, 256, 256])\n",
            "/content/Human_TalkingHead/LIA/networks/styledecoder.py:439: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
            "The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
            "Q, R = torch.qr(A, some)\n",
            "should be replaced with\n",
            "Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2425.)\n",
            "  Q, R = torch.qr(weight)  # get eignvector, orthogonal [n1, n2, n3, n4]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "  0% 1/425 [00:00<03:35,  1.97it/s]Shape of img_target at index 1:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 2:  torch.Size([1, 3, 256, 256])\n",
            "Received timestamp: 16396\n",
            "Analyzing frame...\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "  1% 3/425 [00:00<01:16,  5.50it/s]Shape of img_target at index 3:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 4:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "  1% 5/425 [00:00<00:49,  8.48it/s]Shape of img_target at index 5:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 6:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "  2% 7/425 [00:00<00:40, 10.44it/s]Shape of img_target at index 7:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 8:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "  2% 9/425 [00:01<00:33, 12.34it/s]Shape of img_target at index 9:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 10:  torch.Size([1, 3, 256, 256])\n",
            "Analysis result: [{'emotion': {'angry': 8.70809257030487, 'disgust': 0.0003068043270104681, 'fear': 2.867002971470356, 'happy': 0.04498999333009124, 'sad': 4.4832151383161545, 'surprise': 0.2682628342881799, 'neutral': 83.62812995910645}, 'dominant_emotion': 'neutral', 'region': {'x': 225, 'y': 139, 'w': 208, 'h': 208, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.95}]\n",
            "[17/Jun/2024 08:04:41] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 11:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "  3% 12/425 [00:01<00:26, 15.46it/s]Shape of img_target at index 12:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 13:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 14:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "  4% 15/425 [00:01<00:23, 17.61it/s]Shape of img_target at index 15:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 16:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 17:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "  4% 18/425 [00:01<00:21, 19.16it/s]Shape of img_target at index 18:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 19:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 20:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "  5% 21/425 [00:01<00:19, 20.32it/s]Shape of img_target at index 21:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 22:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 23:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "  6% 24/425 [00:01<00:19, 21.10it/s]Shape of img_target at index 24:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 25:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 26:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "  6% 27/425 [00:01<00:18, 21.64it/s]Shape of img_target at index 27:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 28:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 29:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "  7% 30/425 [00:01<00:17, 22.00it/s]Shape of img_target at index 30:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 31:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 32:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "  8% 33/425 [00:02<00:17, 22.24it/s]Shape of img_target at index 33:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 34:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 35:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "  8% 36/425 [00:02<00:17, 22.43it/s]Shape of img_target at index 36:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 37:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 38:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "  9% 39/425 [00:02<00:17, 22.58it/s]Shape of img_target at index 39:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 40:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 41:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 10% 42/425 [00:02<00:17, 22.50it/s]Shape of img_target at index 42:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 43:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 44:  torch.Size([1, 3, 256, 256])\n",
            "Received timestamp: 18380\n",
            "Analyzing frame...\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 11% 45/425 [00:02<00:16, 22.55it/s]Shape of img_target at index 45:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 46:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 47:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 11% 48/425 [00:02<00:16, 22.37it/s]Shape of img_target at index 48:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 49:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 50:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 12% 51/425 [00:02<00:16, 22.07it/s]Shape of img_target at index 51:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 52:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 53:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 13% 54/425 [00:03<00:17, 20.97it/s]Shape of img_target at index 54:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 55:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 56:  torch.Size([1, 3, 256, 256])\n",
            "Analysis result: [{'emotion': {'angry': 4.959637299180031, 'disgust': 1.5697645494583412e-05, 'fear': 0.7788200862705708, 'happy': 0.011350827844580635, 'sad': 1.38494074344635, 'surprise': 0.015499051369260997, 'neutral': 92.84973740577698}, 'dominant_emotion': 'neutral', 'region': {'x': 215, 'y': 140, 'w': 213, 'h': 213, 'left_eye': (343, 211), 'right_eye': (281, 221)}, 'face_confidence': 0.97}]\n",
            "[17/Jun/2024 08:04:43] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 13% 57/425 [00:03<00:17, 21.13it/s]Shape of img_target at index 57:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 58:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 59:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 14% 60/425 [00:03<00:16, 21.68it/s]Shape of img_target at index 60:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 61:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 62:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 15% 63/425 [00:03<00:16, 21.68it/s]Shape of img_target at index 63:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 64:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 65:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 16% 66/425 [00:03<00:17, 20.32it/s]Shape of img_target at index 66:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 67:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 68:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 16% 69/425 [00:03<00:16, 20.96it/s]Shape of img_target at index 69:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 70:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 71:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 17% 72/425 [00:03<00:16, 21.09it/s]Shape of img_target at index 72:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 73:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 74:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 18% 75/425 [00:04<00:16, 21.52it/s]Shape of img_target at index 75:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 76:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 77:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 18% 78/425 [00:04<00:15, 21.71it/s]Shape of img_target at index 78:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 79:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 80:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 19% 81/425 [00:04<00:15, 21.85it/s]Shape of img_target at index 81:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 82:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 83:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 20% 84/425 [00:04<00:15, 21.96it/s]Shape of img_target at index 84:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 85:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 86:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 20% 87/425 [00:04<00:15, 21.96it/s]Shape of img_target at index 87:  torch.Size([1, 3, 256, 256])\n",
            "Received timestamp: 20391\n",
            "Analyzing frame...\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 88:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 89:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 21% 90/425 [00:04<00:21, 15.28it/s]Shape of img_target at index 90:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 91:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 22% 92/425 [00:04<00:20, 15.91it/s]Shape of img_target at index 92:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 93:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 22% 94/425 [00:05<00:21, 15.42it/s]Shape of img_target at index 94:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 95:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 23% 96/425 [00:05<00:21, 15.49it/s]Shape of img_target at index 96:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 97:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 98:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 23% 99/425 [00:05<00:19, 16.88it/s]Shape of img_target at index 99:  torch.Size([1, 3, 256, 256])\n",
            "Analysis result: [{'emotion': {'angry': 2.084857039153576, 'disgust': 2.7943332270297105e-05, 'fear': 1.5054595656692982, 'happy': 0.04108275752514601, 'sad': 4.406830668449402, 'surprise': 0.01958107459358871, 'neutral': 91.94216132164001}, 'dominant_emotion': 'neutral', 'region': {'x': 211, 'y': 144, 'w': 209, 'h': 209, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.97}]\n",
            "[17/Jun/2024 08:04:45] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 100:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 101:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 24% 102/425 [00:05<00:17, 18.30it/s]Shape of img_target at index 102:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 103:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 104:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 25% 105/425 [00:05<00:16, 19.50it/s]Shape of img_target at index 105:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 106:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 107:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 25% 108/425 [00:05<00:15, 20.03it/s]Shape of img_target at index 108:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 109:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 110:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 26% 111/425 [00:05<00:16, 19.03it/s]Shape of img_target at index 111:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 112:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 27% 113/425 [00:06<00:16, 18.76it/s]Shape of img_target at index 113:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 114:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 115:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 27% 116/425 [00:06<00:15, 19.73it/s]Shape of img_target at index 116:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 117:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 118:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 28% 119/425 [00:06<00:14, 20.55it/s]Shape of img_target at index 119:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 120:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 121:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 29% 122/425 [00:06<00:14, 20.94it/s]Shape of img_target at index 122:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 123:  torch.Size([1, 3, 256, 256])\n",
            "Received timestamp: 22388\n",
            "Analyzing frame...\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 124:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 29% 125/425 [00:06<00:17, 17.54it/s]Shape of img_target at index 125:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 126:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 30% 127/425 [00:06<00:19, 14.92it/s]Shape of img_target at index 127:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 128:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 30% 129/425 [00:07<00:20, 14.55it/s]Shape of img_target at index 129:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 130:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 31% 131/425 [00:07<00:24, 12.03it/s]Shape of img_target at index 131:  torch.Size([1, 3, 256, 256])\n",
            "Analysis result: [{'emotion': {'angry': 10.483759521470041, 'disgust': 0.001457194044177112, 'fear': 2.0085628410317207, 'happy': 0.05896482968120752, 'sad': 5.617827884425394, 'surprise': 0.08242692474771587, 'neutral': 81.74700032156707}, 'dominant_emotion': 'neutral', 'region': {'x': 211, 'y': 146, 'w': 203, 'h': 203, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.96}]\n",
            "[17/Jun/2024 08:04:47] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 132:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 31% 133/425 [00:07<00:23, 12.52it/s]Shape of img_target at index 133:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 134:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 135:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 32% 136/425 [00:07<00:19, 14.82it/s]Shape of img_target at index 136:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 137:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 138:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 33% 139/425 [00:07<00:17, 16.69it/s]Shape of img_target at index 139:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 140:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 141:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 33% 142/425 [00:07<00:15, 17.92it/s]Shape of img_target at index 142:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 143:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 34% 144/425 [00:08<00:15, 18.35it/s]Shape of img_target at index 144:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 145:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 146:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 35% 147/425 [00:08<00:14, 19.31it/s]Shape of img_target at index 147:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 148:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 149:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 35% 150/425 [00:08<00:13, 20.07it/s]Shape of img_target at index 150:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 151:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 152:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 36% 153/425 [00:08<00:13, 19.63it/s]Shape of img_target at index 153:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 154:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 36% 155/425 [00:08<00:14, 19.05it/s]Shape of img_target at index 155:  torch.Size([1, 3, 256, 256])\n",
            "Received timestamp: 24381\n",
            "Analyzing frame...\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 156:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 37% 157/425 [00:08<00:17, 15.53it/s]Shape of img_target at index 157:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 158:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 37% 159/425 [00:08<00:19, 13.58it/s]Shape of img_target at index 159:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 160:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 38% 161/425 [00:09<00:20, 13.15it/s]Shape of img_target at index 161:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 162:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 38% 163/425 [00:09<00:20, 12.96it/s]Shape of img_target at index 163:  torch.Size([1, 3, 256, 256])\n",
            "Analysis result: [{'emotion': {'angry': 43.67188811302185, 'disgust': 0.2481189090758562, 'fear': 12.597349286079407, 'happy': 0.4232162144035101, 'sad': 10.056383162736893, 'surprise': 0.41563608683645725, 'neutral': 32.587411999702454}, 'dominant_emotion': 'angry', 'region': {'x': 211, 'y': 145, 'w': 201, 'h': 201, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.97}]\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 164:  torch.Size([1, 3, 256, 256])\n",
            "[17/Jun/2024 08:04:49] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 165:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 39% 166/425 [00:09<00:16, 15.26it/s]Shape of img_target at index 166:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 167:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 168:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 40% 169/425 [00:09<00:15, 16.96it/s]Shape of img_target at index 169:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 170:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 40% 171/425 [00:09<00:15, 16.32it/s]Shape of img_target at index 171:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 172:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 173:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 41% 174/425 [00:09<00:14, 17.89it/s]Shape of img_target at index 174:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 175:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 176:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 42% 177/425 [00:09<00:13, 18.94it/s]Shape of img_target at index 177:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 178:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 179:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 42% 180/425 [00:10<00:12, 19.43it/s]Shape of img_target at index 180:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 181:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 182:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 43% 183/425 [00:10<00:12, 19.86it/s]Shape of img_target at index 183:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 184:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 44% 185/425 [00:10<00:12, 19.88it/s]Shape of img_target at index 185:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 186:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 44% 187/425 [00:10<00:12, 19.77it/s]Shape of img_target at index 187:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 188:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 189:  torch.Size([1, 3, 256, 256])\n",
            "Received timestamp: 26399\n",
            "Analyzing frame...\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 45% 190/425 [00:10<00:12, 18.17it/s]Shape of img_target at index 190:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 191:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 45% 192/425 [00:10<00:18, 12.42it/s]Shape of img_target at index 192:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 193:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 46% 194/425 [00:11<00:20, 11.16it/s]Shape of img_target at index 194:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 195:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 46% 196/425 [00:11<00:20, 10.99it/s]Shape of img_target at index 196:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 197:  torch.Size([1, 3, 256, 256])\n",
            "Analysis result: [{'emotion': {'angry': 33.37961137294769, 'disgust': 0.37160112988203764, 'fear': 24.484701454639435, 'happy': 0.5802666768431664, 'sad': 9.768648445606232, 'surprise': 0.6797593552619219, 'neutral': 30.73541224002838}, 'dominant_emotion': 'angry', 'region': {'x': 212, 'y': 140, 'w': 202, 'h': 202, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.97}]\n",
            "[17/Jun/2024 08:04:51] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 47% 198/425 [00:11<00:19, 11.72it/s]Shape of img_target at index 198:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 199:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 200:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 47% 201/425 [00:11<00:15, 14.14it/s]Shape of img_target at index 201:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 202:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 203:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 48% 204/425 [00:11<00:13, 16.13it/s]Shape of img_target at index 204:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 205:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 206:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 49% 207/425 [00:11<00:12, 17.63it/s]Shape of img_target at index 207:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 208:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 209:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 49% 210/425 [00:12<00:11, 18.77it/s]Shape of img_target at index 210:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 211:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 50% 212/425 [00:12<00:12, 16.87it/s]Shape of img_target at index 212:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 213:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 214:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 51% 215/425 [00:12<00:11, 17.91it/s]Shape of img_target at index 215:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 216:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 217:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 51% 218/425 [00:12<00:10, 18.93it/s]Shape of img_target at index 218:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 219:  torch.Size([1, 3, 256, 256])\n",
            "Received timestamp: 28391\n",
            "Analyzing frame...\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 52% 220/425 [00:12<00:11, 18.56it/s]Shape of img_target at index 220:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 221:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 52% 222/425 [00:12<00:13, 14.50it/s]Shape of img_target at index 222:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 223:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 53% 224/425 [00:13<00:16, 12.14it/s]Shape of img_target at index 224:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 225:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 53% 226/425 [00:13<00:18, 10.97it/s]Shape of img_target at index 226:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 227:  torch.Size([1, 3, 256, 256])\n",
            "Analysis result: [{'emotion': {'angry': 0.010636330989655107, 'disgust': 2.1728521204522622e-07, 'fear': 41.52476191520691, 'happy': 0.4104885272681713, 'sad': 1.0436203330755234, 'surprise': 0.20007179118692875, 'neutral': 56.81042671203613}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:04:53] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 54% 228/425 [00:13<00:18, 10.57it/s]Shape of img_target at index 228:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 229:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 230:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 54% 231/425 [00:13<00:14, 13.06it/s]Shape of img_target at index 231:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 232:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 233:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 55% 234/425 [00:13<00:12, 15.13it/s]Shape of img_target at index 234:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 235:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 236:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 56% 237/425 [00:13<00:11, 16.71it/s]Shape of img_target at index 237:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 238:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 239:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 56% 240/425 [00:14<00:10, 17.96it/s]Shape of img_target at index 240:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 241:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 242:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 57% 243/425 [00:14<00:09, 18.88it/s]Shape of img_target at index 243:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 244:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 245:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 58% 246/425 [00:14<00:09, 19.25it/s]Shape of img_target at index 246:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 247:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 248:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 59% 249/425 [00:14<00:08, 19.90it/s]Shape of img_target at index 249:  torch.Size([1, 3, 256, 256])\n",
            "Received timestamp: 30383\n",
            "Analyzing frame...\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 250:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 251:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 59% 252/425 [00:14<00:12, 14.10it/s]Shape of img_target at index 252:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 253:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 60% 254/425 [00:15<00:13, 12.38it/s]Shape of img_target at index 254:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 255:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 60% 256/425 [00:15<00:14, 11.83it/s]Shape of img_target at index 256:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 257:  torch.Size([1, 3, 256, 256])\n",
            "Analysis result: [{'emotion': {'angry': 1.577579067379986e-07, 'disgust': 2.1733970417122564e-15, 'fear': 51.64245367050171, 'happy': 0.009237306221621111, 'sad': 48.081302642822266, 'surprise': 3.764320466123107e-12, 'neutral': 0.26700831949710846}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:04:55] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 61% 258/425 [00:15<00:13, 12.49it/s]Shape of img_target at index 258:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 259:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 260:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 61% 261/425 [00:15<00:11, 14.59it/s]Shape of img_target at index 261:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 262:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 263:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 62% 264/425 [00:15<00:09, 16.28it/s]Shape of img_target at index 264:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 265:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 266:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 63% 267/425 [00:15<00:09, 17.38it/s]Shape of img_target at index 267:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 268:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 63% 269/425 [00:16<00:10, 15.35it/s]Shape of img_target at index 269:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 270:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 271:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 64% 272/425 [00:16<00:09, 16.60it/s]Shape of img_target at index 272:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 273:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 274:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 65% 275/425 [00:16<00:08, 17.89it/s]Shape of img_target at index 275:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 276:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 277:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 65% 278/425 [00:16<00:07, 18.97it/s]Shape of img_target at index 278:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 279:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 280:  torch.Size([1, 3, 256, 256])\n",
            "Received timestamp: 32418\n",
            "Analyzing frame...\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 66% 281/425 [00:16<00:07, 18.93it/s]Shape of img_target at index 281:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 282:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 67% 283/425 [00:16<00:09, 15.65it/s]Shape of img_target at index 283:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 284:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 67% 285/425 [00:17<00:10, 13.21it/s]Shape of img_target at index 285:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 286:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 68% 287/425 [00:17<00:11, 12.12it/s]Shape of img_target at index 287:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 288:  torch.Size([1, 3, 256, 256])\n",
            "Analysis result: [{'emotion': {'angry': 0.001623612297407817, 'disgust': 3.505710999274214e-09, 'fear': 98.83954524993896, 'happy': 0.01658104156376794, 'sad': 1.0676553472876549, 'surprise': 0.04800492024514824, 'neutral': 0.02658864250406623}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:04:57] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 68% 289/425 [00:17<00:10, 12.94it/s]Shape of img_target at index 289:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 290:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 291:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 69% 292/425 [00:17<00:08, 15.23it/s]Shape of img_target at index 292:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 293:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 294:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 69% 295/425 [00:17<00:07, 17.11it/s]Shape of img_target at index 295:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 296:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 297:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 70% 298/425 [00:17<00:06, 18.56it/s]Shape of img_target at index 298:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 299:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 300:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 71% 301/425 [00:17<00:06, 19.73it/s]Shape of img_target at index 301:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 302:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 303:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 72% 304/425 [00:18<00:05, 20.63it/s]Shape of img_target at index 304:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 305:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 306:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 72% 307/425 [00:18<00:05, 21.31it/s]Shape of img_target at index 307:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 308:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 309:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 73% 310/425 [00:18<00:05, 21.70it/s]Shape of img_target at index 310:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 311:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 312:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 74% 313/425 [00:18<00:05, 21.77it/s]Shape of img_target at index 313:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 314:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 315:  torch.Size([1, 3, 256, 256])\n",
            "Received timestamp: 34381\n",
            "Analyzing frame...\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 74% 316/425 [00:18<00:05, 21.49it/s]Shape of img_target at index 316:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 317:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 318:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 75% 319/425 [00:18<00:04, 21.26it/s]Shape of img_target at index 319:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 320:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 321:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 76% 322/425 [00:18<00:05, 19.14it/s]Shape of img_target at index 322:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 323:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 76% 324/425 [00:19<00:05, 19.07it/s]Shape of img_target at index 324:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 325:  torch.Size([1, 3, 256, 256])\n",
            "Analysis result: [{'emotion': {'angry': 1.1905913019532741, 'disgust': 1.4997962877402183e-06, 'fear': 0.40302027556421016, 'happy': 0.01594276204270941, 'sad': 0.5843923775489556, 'surprise': 0.15936498315945166, 'neutral': 97.64668927470977}, 'dominant_emotion': 'neutral', 'region': {'x': 244, 'y': 152, 'w': 173, 'h': 173, 'left_eye': (349, 208), 'right_eye': (298, 215)}, 'face_confidence': 0.97}]\n",
            "[17/Jun/2024 08:04:59] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 326:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 77% 327/425 [00:19<00:04, 19.95it/s]Shape of img_target at index 327:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 328:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 329:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 78% 330/425 [00:19<00:04, 20.82it/s]Shape of img_target at index 330:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 331:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 332:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 78% 333/425 [00:19<00:04, 21.36it/s]Shape of img_target at index 333:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 334:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 335:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 79% 336/425 [00:19<00:04, 21.17it/s]Shape of img_target at index 336:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 337:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 338:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 80% 339/425 [00:19<00:03, 21.52it/s]Shape of img_target at index 339:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 340:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 341:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 80% 342/425 [00:19<00:03, 21.93it/s]Shape of img_target at index 342:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 343:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 344:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 81% 345/425 [00:20<00:03, 22.20it/s]Shape of img_target at index 345:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 346:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 347:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 82% 348/425 [00:20<00:03, 22.41it/s]Shape of img_target at index 348:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 349:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 350:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 83% 351/425 [00:20<00:03, 22.61it/s]Shape of img_target at index 351:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 352:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 353:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 83% 354/425 [00:20<00:03, 22.57it/s]Shape of img_target at index 354:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 355:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 356:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 84% 357/425 [00:20<00:02, 22.70it/s]Shape of img_target at index 357:  torch.Size([1, 3, 256, 256])\n",
            "Received timestamp: 36387\n",
            "Analyzing frame...\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 358:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 359:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 85% 360/425 [00:20<00:03, 21.46it/s]Shape of img_target at index 360:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 361:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 362:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 85% 363/425 [00:20<00:03, 17.89it/s]Shape of img_target at index 363:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 364:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 86% 365/425 [00:21<00:03, 18.04it/s]Shape of img_target at index 365:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 366:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 367:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 87% 368/425 [00:21<00:03, 18.89it/s]Shape of img_target at index 368:  torch.Size([1, 3, 256, 256])\n",
            "Analysis result: [{'emotion': {'angry': 45.29833467592804, 'disgust': 0.0003178601408137931, 'fear': 0.7120551500204295, 'happy': 5.5772382365011115, 'sad': 0.7693251122132011, 'surprise': 17.545881423108664, 'neutral': 30.09684861283592}, 'dominant_emotion': 'angry', 'region': {'x': 32, 'y': 142, 'w': 65, 'h': 65, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.94}, {'emotion': {'angry': 1.709046689215105, 'disgust': 1.6575082917156244e-05, 'fear': 0.35858619767196165, 'happy': 0.021379958890208387, 'sad': 1.103762975559412, 'surprise': 0.00892101359990879, 'neutral': 96.7982886313372}, 'dominant_emotion': 'neutral', 'region': {'x': 231, 'y': 127, 'w': 205, 'h': 205, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.98}]\n",
            "[17/Jun/2024 08:05:01] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 369:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 370:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 87% 371/425 [00:21<00:02, 19.91it/s]Shape of img_target at index 371:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 372:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 373:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 88% 374/425 [00:21<00:02, 20.80it/s]Shape of img_target at index 374:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 375:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 376:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 89% 377/425 [00:21<00:02, 21.37it/s]Shape of img_target at index 377:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 378:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 379:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 89% 380/425 [00:21<00:02, 21.77it/s]Shape of img_target at index 380:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 381:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 382:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 90% 383/425 [00:21<00:01, 21.65it/s]Shape of img_target at index 383:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 384:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 385:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 91% 386/425 [00:21<00:01, 21.87it/s]Shape of img_target at index 386:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 387:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 388:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 92% 389/425 [00:22<00:01, 22.08it/s]Shape of img_target at index 389:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 390:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 391:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 92% 392/425 [00:22<00:01, 22.27it/s]Shape of img_target at index 392:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 393:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 394:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 93% 395/425 [00:22<00:01, 22.41it/s]Shape of img_target at index 395:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 396:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 397:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 94% 398/425 [00:22<00:01, 22.51it/s]Shape of img_target at index 398:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 399:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 400:  torch.Size([1, 3, 256, 256])\n",
            "Received timestamp: 38382\n",
            "Analyzing frame...\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 94% 401/425 [00:22<00:01, 22.53it/s]Shape of img_target at index 401:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 402:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 403:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 95% 404/425 [00:22<00:00, 22.13it/s]Shape of img_target at index 404:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 405:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 406:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 96% 407/425 [00:22<00:00, 20.57it/s]Shape of img_target at index 407:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 408:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 409:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 96% 410/425 [00:23<00:00, 19.59it/s]Shape of img_target at index 410:  torch.Size([1, 3, 256, 256])\n",
            "Analysis result: [{'emotion': {'angry': 3.5933196544647217, 'disgust': 0.0009177869287668727, 'fear': 46.65394127368927, 'happy': 0.03202245570719242, 'sad': 39.60546851158142, 'surprise': 0.34633514005690813, 'neutral': 9.767996519804}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:05:03] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 411:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 412:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 97% 413/425 [00:23<00:00, 20.44it/s]Shape of img_target at index 413:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 414:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 415:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 98% 416/425 [00:23<00:00, 20.97it/s]Shape of img_target at index 416:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 417:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 418:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 99% 419/425 [00:23<00:00, 21.46it/s]Shape of img_target at index 419:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 420:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 421:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            " 99% 422/425 [00:23<00:00, 21.72it/s]Shape of img_target at index 422:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 423:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_target at index 424:  torch.Size([1, 3, 256, 256])\n",
            "Shape of img_recon:  torch.Size([1, 3, 256, 256])\n",
            "100% 425/425 [00:23<00:00, 17.88it/s]\n",
            "Received timestamp: 40383\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 12.466142326593399, 'disgust': 0.015024350432213396, 'fear': 63.888758420944214, 'happy': 0.006362577551044524, 'sad': 15.36577045917511, 'surprise': 0.10527072008699179, 'neutral': 8.152678608894348}, 'dominant_emotion': 'fear', 'region': {'x': 303, 'y': 161, 'w': 156, 'h': 156, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.95}, {'emotion': {'angry': 0.8787532860787681, 'disgust': 1.0842785885500066, 'fear': 13.877844147553692, 'happy': 0.06360136700839501, 'sad': 82.75259034351053, 'surprise': 0.0005680061687824967, 'neutral': 1.342365691355806}, 'dominant_emotion': 'sad', 'region': {'x': 138, 'y': 308, 'w': 59, 'h': 59, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.96}]\n",
            "[17/Jun/2024 08:05:05] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            "Received timestamp: 42385\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 13.552017509937286, 'disgust': 0.10281599825248122, 'fear': 57.668256759643555, 'happy': 0.010946766997221857, 'sad': 16.623874008655548, 'surprise': 0.05906582809984684, 'neutral': 11.98301613330841}, 'dominant_emotion': 'fear', 'region': {'x': 248, 'y': 172, 'w': 153, 'h': 153, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.96}]\n",
            "[17/Jun/2024 08:05:07] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            "Received timestamp: 44386\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 15.49583524465561, 'disgust': 7.225967380009024e-06, 'fear': 1.566401869058609, 'happy': 0.6304272916167974, 'sad': 12.02225461602211, 'surprise': 0.7108887657523155, 'neutral': 69.57418918609619}, 'dominant_emotion': 'neutral', 'region': {'x': 33, 'y': 142, 'w': 65, 'h': 65, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.93}, {'emotion': {'angry': 3.4080374985933304, 'disgust': 0.24667037650942802, 'fear': 5.282891169190407, 'happy': 60.19864082336426, 'sad': 4.803860187530518, 'surprise': 9.637145698070526, 'neutral': 16.422756016254425}, 'dominant_emotion': 'happy', 'region': {'x': 258, 'y': 179, 'w': 144, 'h': 144, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.97}]\n",
            "[17/Jun/2024 08:05:09] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 46395\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 55.81062436103821, 'disgust': 1.6974719585505227e-05, 'fear': 0.409168004989624, 'happy': 1.0873922146856785, 'sad': 0.46340874396264553, 'surprise': 10.668186098337173, 'neutral': 31.56120479106903}, 'dominant_emotion': 'angry', 'region': {'x': 32, 'y': 142, 'w': 63, 'h': 63, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.94}, {'emotion': {'angry': 25.72924792766571, 'disgust': 0.3724898910149932, 'fear': 15.318669378757477, 'happy': 0.6671338807791471, 'sad': 14.373192191123962, 'surprise': 0.409038458019495, 'neutral': 43.13023090362549}, 'dominant_emotion': 'neutral', 'region': {'x': 249, 'y': 181, 'w': 148, 'h': 148, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.98}]\n",
            "[17/Jun/2024 08:05:12] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 48389\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 51.61069054731018, 'disgust': 1.9591166226612894e-05, 'fear': 0.6501116167886992, 'happy': 3.2813336886565314, 'sad': 2.3343252938181513, 'surprise': 2.8181461251552253, 'neutral': 39.3053787118648}, 'dominant_emotion': 'angry', 'region': {'x': 32, 'y': 142, 'w': 64, 'h': 64, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.92}, {'emotion': {'angry': 20.817073188176966, 'disgust': 0.3746443214318965, 'fear': 24.263236014457224, 'happy': 1.3633202998852372, 'sad': 19.985466303173745, 'surprise': 1.998929254838317, 'neutral': 31.197326636633083}, 'dominant_emotion': 'neutral', 'region': {'x': 246, 'y': 172, 'w': 145, 'h': 145, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.93}]\n",
            "[17/Jun/2024 08:05:13] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 50387\n",
            "Analyzing frame...\n",
            "Motion transfer complete. Video saved.\n",
            "[17/Jun/2024 08:05:15] \u001b[m\"POST / HTTP/1.1\" 200 12767\u001b[0m\n",
            "Analysis result: [{'emotion': {'angry': 64.74346098225844, 'disgust': 7.978251615645964e-05, 'fear': 0.9314072497376501, 'happy': 2.019805329418818, 'sad': 1.876892186926864, 'surprise': 7.841485928474179, 'neutral': 22.586875866279108}, 'dominant_emotion': 'angry', 'region': {'x': 32, 'y': 142, 'w': 64, 'h': 64, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.94}, {'emotion': {'angry': 27.53155529499054, 'disgust': 1.1419634334743023, 'fear': 32.78135359287262, 'happy': 17.268480360507965, 'sad': 6.60807341337204, 'surprise': 5.416535958647728, 'neutral': 9.252039343118668}, 'dominant_emotion': 'fear', 'region': {'x': 276, 'y': 175, 'w': 150, 'h': 150, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.94}]\n",
            "[17/Jun/2024 08:05:15] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 52389\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 1.4288239181041718, 'disgust': 0.12917681597173214, 'fear': 5.848048627376556, 'happy': 65.95240831375122, 'sad': 4.95029054582119, 'surprise': 4.0436554700136185, 'neutral': 17.64759123325348}, 'dominant_emotion': 'happy', 'region': {'x': 254, 'y': 178, 'w': 143, 'h': 143, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.97}]\n",
            "Received timestamp: 2034\n",
            "Received timestamp: 1\n",
            "Received timestamp: 1014\n",
            "Analyzing frame...\n",
            "Analyzing frame...\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 25.941869616508484, 'disgust': 0.007751990051474422, 'fear': 3.172805532813072, 'happy': 42.306798696517944, 'sad': 2.0702293142676353, 'surprise': 4.242289438843727, 'neutral': 22.258253395557404}, 'dominant_emotion': 'happy', 'region': {'x': 237, 'y': 166, 'w': 149, 'h': 149, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.97}]\n",
            "[17/Jun/2024 08:05:20] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 3042\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 39.706721901893616, 'disgust': 0.00013993901575304335, 'fear': 14.36336487531662, 'happy': 0.5150581244379282, 'sad': 0.6918601226061583, 'surprise': 13.540956377983093, 'neutral': 31.181898713111877}, 'dominant_emotion': 'angry', 'region': {'x': 233, 'y': 164, 'w': 153, 'h': 153, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.97}]\n",
            "[17/Jun/2024 08:05:21] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Analysis result: [{'emotion': {'angry': 81.25018365682266, 'disgust': 5.711539980112998e-11, 'fear': 0.8020354912898238, 'happy': 1.7191729065861778e-07, 'sad': 9.423801864612214, 'surprise': 1.1468286726600225e-08, 'neutral': 8.523982526301296}, 'dominant_emotion': 'angry', 'region': {'x': 217, 'y': 159, 'w': 160, 'h': 160, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.95}]\n",
            "[17/Jun/2024 08:05:21] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Analysis result: [{'emotion': {'angry': 0.01196010853163898, 'disgust': 1.3533001975984948e-09, 'fear': 0.00789098849054426, 'happy': 0.0022449457901529968, 'sad': 0.07762399618513882, 'surprise': 0.0008887705007509794, 'neutral': 99.89939332008362}, 'dominant_emotion': 'neutral', 'region': {'x': 217, 'y': 159, 'w': 160, 'h': 160, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.95}]\n",
            "[17/Jun/2024 08:05:21] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 829\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 4.179698601365089, 'disgust': 1.8001699686465145e-06, 'fear': 0.4867424722760916, 'happy': 0.7720831781625748, 'sad': 2.748383767902851, 'surprise': 0.666119297966361, 'neutral': 91.14697575569153}, 'dominant_emotion': 'neutral', 'region': {'x': 34, 'y': 144, 'w': 62, 'h': 62, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.92}, {'emotion': {'angry': 0.033282212098129094, 'disgust': 2.0634471908920204e-08, 'fear': 0.08382561500184238, 'happy': 0.00905899578356184, 'sad': 0.18380875699222088, 'surprise': 0.053974264301359653, 'neutral': 99.63604807853699}, 'dominant_emotion': 'neutral', 'region': {'x': 224, 'y': 157, 'w': 150, 'h': 150, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.92}]\n",
            "[17/Jun/2024 08:05:22] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 1833\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.004142241829929916, 'disgust': 9.981121726174397e-11, 'fear': 0.01798794914273779, 'happy': 0.007179609982771727, 'sad': 0.2248086465750758, 'surprise': 0.012465093428704926, 'neutral': 99.73341229755732}, 'dominant_emotion': 'neutral', 'region': {'x': 230, 'y': 156, 'w': 146, 'h': 146, 'left_eye': (316, 206), 'right_eye': (271, 211)}, 'face_confidence': 0.93}]\n",
            "[17/Jun/2024 08:05:23] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 2837\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.008288719254778698, 'disgust': 1.8726097061882996e-09, 'fear': 0.11499964166432619, 'happy': 0.022250195615924895, 'sad': 1.1559389531612396, 'surprise': 0.0021817186279804446, 'neutral': 98.69634509086609}, 'dominant_emotion': 'neutral', 'region': {'x': 244, 'y': 161, 'w': 137, 'h': 137, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.94}]\n",
            "[17/Jun/2024 08:05:24] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 3847\n",
            "Analyzing frame...\n",
            "Received audio file\n",
            "Saved audio file as temp_audio.webm\n",
            "Analysis result: [{'emotion': {'angry': 0.002267895615659654, 'disgust': 3.014190459726951e-11, 'fear': 0.006661788211204112, 'happy': 0.0007770699085085653, 'sad': 0.24814903736114502, 'surprise': 0.000290595312435471, 'neutral': 99.74185228347778}, 'dominant_emotion': 'neutral', 'region': {'x': 224, 'y': 154, 'w': 149, 'h': 149, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.94}]\n",
            "[17/Jun/2024 08:05:25] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "Input #0, matroska,webm, from 'temp_audio_session-1u4hv44lc.webm':\n",
            "  Metadata:\n",
            "    encoder         : Chrome\n",
            "  Duration: N/A, start: 0.000000, bitrate: N/A\n",
            "  Stream #0:0(eng): Audio: opus, 48000 Hz, mono, fltp (default)\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (opus (native) -> pcm_s16le (native))\n",
            "Press [q] to stop, [?] for help\n",
            "Output #0, wav, to 'temp_audio_session-1u4hv44lc.wav':\n",
            "  Metadata:\n",
            "    ISFT            : Lavf58.76.100\n",
            "  Stream #0:0(eng): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, mono, s16, 768 kb/s (default)\n",
            "    Metadata:\n",
            "      encoder         : Lavc58.134.100 pcm_s16le\n",
            "size=     270kB time=00:00:02.81 bitrate= 784.7kbits/s speed=89.2x    \n",
            "video:0kB audio:270kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.028212%\n",
            "Converted audio file to temp_audio_session-1u4hv44lc.wav\n",
            "Audio file format: 1 channels, 2 bytes per sample, 48000 Hz\n",
            "Received timestamp: 4866\n",
            "Analyzing frame...\n",
            "Transcription: hello hello hello hello\n",
            "Analysis result: [{'emotion': {'angry': 0.005298987841592377, 'disgust': 4.494646242752512e-10, 'fear': 0.06418395005890429, 'happy': 0.005901049723189494, 'sad': 0.18014993893876308, 'surprise': 0.0026099536362798256, 'neutral': 99.74185822855581}, 'dominant_emotion': 'neutral', 'region': {'x': 236, 'y': 169, 'w': 133, 'h': 133, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.93}]\n",
            "[17/Jun/2024 08:05:26] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Tone features: [[ 5.12713631e-02  1.78385030e-05  1.85796880e+00 ...  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [ 9.00375156e-02  8.59909442e-05  2.91847771e+00 ...  8.65711685e-03\n",
            "  -5.83031378e-03  1.39905774e-02]\n",
            " [ 8.17007086e-02  1.10251496e-04  3.23114242e+00 ...  3.79171628e-02\n",
            "   9.65233215e-03 -6.65732409e-03]\n",
            " ...\n",
            " [ 1.58399333e-01  8.24602369e-05  3.24515016e+00 ... -8.79831421e-04\n",
            "   4.24953223e-03  6.71222570e-03]\n",
            " [ 1.05460609e-01  6.96715070e-05  3.27034732e+00 ...  1.61694680e-02\n",
            "   6.15998848e-03  1.59690522e-03]\n",
            " [ 8.33680700e-02  7.11131091e-05  3.26730679e+00 ...  1.83711080e-02\n",
            "  -1.07387023e-02  1.73987724e-03]]\n",
            "Length mismatch between tone features and timestamps\n",
            "Tone emotions: [(4251, 'sad')]\n",
            "Facial emotions: [{'timestamp': 829.0, 'emotion': 'neutral'}, {'timestamp': 1833.0, 'emotion': 'neutral'}, {'timestamp': 2837.0, 'emotion': 'neutral'}, {'timestamp': 3847.0, 'emotion': 'neutral'}, {'timestamp': 4866.0, 'emotion': 'neutral'}]\n",
            "Transcription words: ['hello', 'hello', 'hello', 'hello']\n",
            "Tone emotions: [(4251, 'sad')]\n",
            "Facial emotions: [{'timestamp': 829.0, 'emotion': 'neutral'}, {'timestamp': 1833.0, 'emotion': 'neutral'}, {'timestamp': 2837.0, 'emotion': 'neutral'}, {'timestamp': 3847.0, 'emotion': 'neutral'}, {'timestamp': 4866.0, 'emotion': 'neutral'}]\n",
            "Word: hello, Word Timestamp: 0\n",
            "Word: hello, Word Timestamp: 250\n",
            "Word: hello, Word Timestamp: 500\n",
            "Word: hello, Word Timestamp: 750\n",
            "Segments: [{'text': '{neutral, neutral} hello hello hello hello', 'timestamp': 0, 'facial_emotion': 'neutral', 'tone_emotion': 'neutral'}]\n",
            "Segmented transcriptions: [{'text': '{neutral, neutral} hello hello hello hello', 'timestamp': 0, 'facial_emotion': 'neutral', 'tone_emotion': 'neutral'}]\n",
            "LLM Response: Analysis: The patient's greeting seems to be a repetitive and excessive greeting, which might be a defense mechanism to avoid engaging in a deeper conversation. The neutral tone and emotion suggest that the patient is not trying to convey any strong emotions or feelings at this point.\n",
            "\n",
            "Therapist: Ah, hi there! *pauses* It's nice to see you're comfortable enough to greet me with a big hello! *smiles* Can you tell me what's really on your mind today?\n",
            "Therapist Reply: Ah, hi there! *pauses* It's nice to see you're comfortable enough to greet me with a big hello! *smiles* Can you tell me what's really on your mind today?\n",
            "Received timestamp: 6681\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.022350337530951947, 'disgust': 6.775528488961458e-10, 'fear': 0.004523251118371263, 'happy': 0.0008690622962603811, 'sad': 0.31089496333152056, 'surprise': 9.93505011592788e-06, 'neutral': 99.66135621070862}, 'dominant_emotion': 'neutral', 'region': {'x': 236, 'y': 168, 'w': 164, 'h': 164, 'left_eye': (340, 223), 'right_eye': (291, 225)}, 'face_confidence': 0.97}]\n",
            "[17/Jun/2024 08:05:28] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 8684\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.02170181962560532, 'disgust': 8.848595994876285e-10, 'fear': 0.006265664351873583, 'happy': 0.0023174273809444647, 'sad': 0.06223216558141503, 'surprise': 0.002481181911759211, 'neutral': 99.90500211149467}, 'dominant_emotion': 'neutral', 'region': {'x': 233, 'y': 155, 'w': 160, 'h': 160, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.95}]\n",
            "[17/Jun/2024 08:05:30] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 9692\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.09526365902274847, 'disgust': 3.594265773898542e-07, 'fear': 0.052663072710856795, 'happy': 0.009350473555969074, 'sad': 1.1798011139035225, 'surprise': 0.002388655047980137, 'neutral': 98.66053462028503}, 'dominant_emotion': 'neutral', 'region': {'x': 228, 'y': 162, 'w': 140, 'h': 140, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.94}]\n",
            "[17/Jun/2024 08:05:31] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 10705\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.16323536210419906, 'disgust': 1.7302484921399947e-07, 'fear': 0.09192463442047742, 'happy': 0.005350583842897197, 'sad': 2.6535732390626214, 'surprise': 0.0004844387892700802, 'neutral': 97.08543402465784}, 'dominant_emotion': 'neutral', 'region': {'x': 222, 'y': 158, 'w': 151, 'h': 151, 'left_eye': (314, 206), 'right_eye': (268, 213)}, 'face_confidence': 0.96}]\n",
            "[17/Jun/2024 08:05:32] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 11716\n",
            "Analyzing frame...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Analysis result: [{'emotion': {'angry': 0.04173170558254156, 'disgust': 2.2397738333423312e-07, 'fear': 0.0671730416818286, 'happy': 0.03126193955611822, 'sad': 0.5082462285903828, 'surprise': 0.0046122709157041634, 'neutral': 99.34697743338212}, 'dominant_emotion': 'neutral', 'region': {'x': 223, 'y': 160, 'w': 145, 'h': 145, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.97}]\n",
            "[17/Jun/2024 08:05:33] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 12719\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.018612135318107903, 'disgust': 2.0095428793220904e-09, 'fear': 0.024193058197852224, 'happy': 0.004909697236143984, 'sad': 0.7364132441580296, 'surprise': 6.362461135722697e-05, 'neutral': 99.2158055305481}, 'dominant_emotion': 'neutral', 'region': {'x': 233, 'y': 166, 'w': 151, 'h': 151, 'left_eye': (323, 216), 'right_eye': (280, 223)}, 'face_confidence': 0.95}]\n",
            "[17/Jun/2024 08:05:34] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 13736\n",
            "Analyzing frame...\n",
            "Received timestamp: 14753\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.0034374988899799064, 'disgust': 1.4892010151076884e-11, 'fear': 0.001267441621166654, 'happy': 0.0006606149327126332, 'sad': 0.01959944056579843, 'surprise': 0.00024556234166084323, 'neutral': 99.97479319572449}, 'dominant_emotion': 'neutral', 'region': {'x': 270, 'y': 198, 'w': 134, 'h': 134, 'left_eye': (354, 244), 'right_eye': (314, 247)}, 'face_confidence': 0.95}]\n",
            "[17/Jun/2024 08:05:35] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Analysis result: [{'emotion': {'angry': 0.0013728060366702266, 'disgust': 5.427379872564517e-12, 'fear': 0.021210052364040166, 'happy': 0.004871784403803758, 'sad': 0.09760616230778396, 'surprise': 0.003398576518520713, 'neutral': 99.87154006958008}, 'dominant_emotion': 'neutral', 'region': {'x': 289, 'y': 183, 'w': 142, 'h': 142, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.93}]\n",
            "[17/Jun/2024 08:05:36] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 16691\n",
            "Analyzing frame...\n",
            "177\n",
            "Analysis result: [{'emotion': {'angry': 3.9848543909037915e-08, 'disgust': 5.6799131002006995e-15, 'fear': 2.1168633068668896e-06, 'happy': 0.0011356787581462413, 'sad': 0.00014807516208747984, 'surprise': 4.887801452468921e-06, 'neutral': 99.99871253967285}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:05:38] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 18692\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.013747438788414001, 'disgust': 1.6639223332504116e-06, 'fear': 0.27465252205729485, 'happy': 0.014423207903746516, 'sad': 0.17033626791089773, 'surprise': 1.2503882870078087, 'neutral': 98.27645421028137}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:05:40] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 20689\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.013091297296341509, 'disgust': 6.156804399637394e-07, 'fear': 0.6897824350744486, 'happy': 0.0185540018719621, 'sad': 0.24220256600528955, 'surprise': 0.9209319949150085, 'neutral': 98.11543822288513}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:05:42] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 21749\n",
            "Analyzing frame...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "Analysis result: [{'emotion': {'angry': 0.003238165663788095, 'disgust': 5.5175342072999456e-08, 'fear': 0.2905415138229728, 'happy': 0.006150575791252777, 'sad': 0.2995752263814211, 'surprise': 0.14341447968035936, 'neutral': 99.25708174705505}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:05:43] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 22762\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.0520082303257161, 'disgust': 1.4041009169083597e-05, 'fear': 0.8738985174717286, 'happy': 0.008317758065254397, 'sad': 0.24988282752688795, 'surprise': 2.348332199653125, 'neutral': 96.4675430694171}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:05:44] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 23772\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.10739699937403202, 'disgust': 1.0286224494393537e-05, 'fear': 0.12549288803711534, 'happy': 0.030239805346354842, 'sad': 0.036365975392982364, 'surprise': 3.648471087217331, 'neutral': 96.05202078819275}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:05:45] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 24787\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.25454589631408453, 'disgust': 4.1085929103346075e-08, 'fear': 11.970958858728409, 'happy': 0.006089002272346988, 'sad': 37.971031665802, 'surprise': 0.002411962668702472, 'neutral': 49.794963002204895}, 'dominant_emotion': 'neutral', 'region': {'x': 271, 'y': 148, 'w': 155, 'h': 155, 'left_eye': (363, 200), 'right_eye': (315, 209)}, 'face_confidence': 0.96}]\n",
            "[17/Jun/2024 08:05:46] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 25802\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 4.068412748077337e-12, 'disgust': 4.0137048107609104e-23, 'fear': 0.00019679418983287178, 'happy': 4.559014554761376e-08, 'sad': 1.4119724743068218, 'surprise': 1.3456835472325235e-10, 'neutral': 98.58783483505249}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:05:47] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 26806\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.008656635327497497, 'disgust': 1.8456267330407838e-10, 'fear': 0.010663655848475173, 'happy': 6.761503641428135e-05, 'sad': 1.5597221441566944, 'surprise': 4.645670870218055e-06, 'neutral': 98.4208881855011}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:05:48] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 27810\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 55.08847772020367, 'disgust': 0.00011796043694511319, 'fear': 0.9200007489322723, 'happy': 3.1237810851596493, 'sad': 0.5567982594760347, 'surprise': 26.77372076824656, 'neutral': 13.53709089313854}, 'dominant_emotion': 'angry', 'region': {'x': 33, 'y': 143, 'w': 62, 'h': 62, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.92}, {'emotion': {'angry': 0.11541468556970358, 'disgust': 2.2559133727728664e-08, 'fear': 0.130261923186481, 'happy': 0.0002565575641710893, 'sad': 0.1809032866731286, 'surprise': 0.0006881384706503013, 'neutral': 99.57247972488403}, 'dominant_emotion': 'neutral', 'region': {'x': 259, 'y': 201, 'w': 134, 'h': 134, 'left_eye': (339, 246), 'right_eye': (298, 255)}, 'face_confidence': 0.95}]\n",
            "[17/Jun/2024 08:05:49] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 29683\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.00311155708914157, 'disgust': 4.913434025963226e-11, 'fear': 0.019014392455574125, 'happy': 0.0019748669728869572, 'sad': 0.057335238670930266, 'surprise': 0.0005735090326197678, 'neutral': 99.91798996925354}, 'dominant_emotion': 'neutral', 'region': {'x': 240, 'y': 251, 'w': 111, 'h': 111, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.96}]\n",
            "[17/Jun/2024 08:05:51] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "bert loaded\n",
            "bert_encoder loaded\n",
            "predictor loaded\n",
            "decoder loaded\n",
            "text_encoder loaded\n",
            "predictor_encoder loaded\n",
            "style_encoder loaded\n",
            "diffusion loaded\n",
            "text_aligner loaded\n",
            "pitch_extractor loaded\n",
            "mpd loaded\n",
            "msd loaded\n",
            "wd loaded\n",
            "Received timestamp: 31687\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.5551191377710782, 'disgust': 3.1391279714861324e-05, 'fear': 2.208673154334614, 'happy': 0.05744184360492787, 'sad': 6.63447270598845, 'surprise': 0.0012511179938822067, 'neutral': 90.54301324743112}, 'dominant_emotion': 'neutral', 'region': {'x': 280, 'y': 207, 'w': 144, 'h': 144, 'left_eye': (371, 257), 'right_eye': (323, 255)}, 'face_confidence': 0.94}]\n",
            "[17/Jun/2024 08:05:53] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "RTF = 0.159037\n",
            "Audio saved to /content/Human_TalkingHead/media/audio/session-1u4hv44lc_response.wav\n",
            "Received timestamp: 33682\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.02846416609827429, 'disgust': 6.834131593969062e-08, 'fear': 49.12886321544647, 'happy': 0.00017982227973334375, 'sad': 17.169851064682007, 'surprise': 1.8655960332125687e-05, 'neutral': 33.672621846199036}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "Audio saved to /content/Human_TalkingHead/media/audio/session-1u4hv44lc_response.wav\n",
            "[17/Jun/2024 08:05:55] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            "Received timestamp: 34686\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 1.1686736187040827e-05, 'disgust': 5.6993296945186765e-11, 'fear': 42.167719731791, 'happy': 1.3367548976256162e-07, 'sad': 57.80168520222218, 'surprise': 1.0951101789760998e-10, 'neutral': 0.03058370676111172}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:05:56] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 17\u001b[0m\n",
            "Received timestamp: 36682\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 3.0078312382102013, 'disgust': 8.036224130592018e-06, 'fear': 2.9104989022016525, 'happy': 0.006037212733644992, 'sad': 8.887746185064316, 'surprise': 0.00842425724840723, 'neutral': 85.17945408821106}, 'dominant_emotion': 'neutral', 'region': {'x': 284, 'y': 227, 'w': 104, 'h': 104, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.96}]\n",
            "[17/Jun/2024 08:05:58] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "add ffmpeg to path\n",
            "Received timestamp: 38684\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.014736143353500566, 'disgust': 1.6927073599754365e-09, 'fear': 0.21244710180593143, 'happy': 0.002323206492522932, 'sad': 0.6110364935419854, 'surprise': 0.00028085327667759003, 'neutral': 99.15917510742993}, 'dominant_emotion': 'neutral', 'region': {'x': 288, 'y': 223, 'w': 114, 'h': 114, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.96}]\n",
            "[17/Jun/2024 08:06:00] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 40699\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 1.395790901027483, 'disgust': 6.624538902940316e-05, 'fear': 98.22859732423973, 'happy': 0.0013540162091980337, 'sad': 0.34856376417376356, 'surprise': 0.0012163916282155016, 'neutral': 0.024413014761916207}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:06:02] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            "Received timestamp: 42685\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.8348440751433372, 'disgust': 8.998077873911825e-05, 'fear': 30.122902989387512, 'happy': 0.005224960477789864, 'sad': 65.52525758743286, 'surprise': 0.00019882454580510966, 'neutral': 3.51148284971714}, 'dominant_emotion': 'sad', 'region': {'x': 301, 'y': 235, 'w': 107, 'h': 107, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.98}]\n",
            "[17/Jun/2024 08:06:04] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 17\u001b[0m\n",
            "Received timestamp: 44689\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.0968207255937159, 'disgust': 2.018594358332848e-06, 'fear': 13.516436517238617, 'happy': 0.007106961129466072, 'sad': 12.347526103258133, 'surprise': 0.0018187964087701403, 'neutral': 74.03029203414917}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:06:06] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 46688\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 23.99956429002072, 'disgust': 8.263106247710249e-07, 'fear': 0.35225057280067046, 'happy': 0.22277148836979202, 'sad': 6.0461602983570275, 'surprise': 0.10808965369848333, 'neutral': 69.27116330094118}, 'dominant_emotion': 'neutral', 'region': {'x': 34, 'y': 143, 'w': 62, 'h': 62, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.91}]\n",
            "[17/Jun/2024 08:06:09] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 48691\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.024969776793260658, 'disgust': 2.7777269358196146e-10, 'fear': 0.11162804273107138, 'happy': 0.00035815510551028093, 'sad': 0.34798223567965797, 'surprise': 4.182162810444021e-05, 'neutral': 99.51501494467081}, 'dominant_emotion': 'neutral', 'region': {'x': 257, 'y': 224, 'w': 109, 'h': 109, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.95}]\n",
            "[17/Jun/2024 08:06:11] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Loads checkpoint by local backend from path: ./models/dwpose/dw-ll_ucoco_384.pth\n",
            "cuda start\n",
            "Received timestamp: 50699\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.19321476562483703, 'disgust': 5.124693153313174e-06, 'fear': 2.021824689020198, 'happy': 0.02621711620864965, 'sad': 1.0563837062647168, 'surprise': 0.011914218321917552, 'neutral': 96.69043998065233}, 'dominant_emotion': 'neutral', 'region': {'x': 242, 'y': 241, 'w': 111, 'h': 111, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.95}]\n",
            "[17/Jun/2024 08:06:13] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 52686\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 5.456584786583806, 'disgust': 0.022294937187105453, 'fear': 73.98667645613236, 'happy': 0.013229093625374153, 'sad': 17.074067345372754, 'surprise': 0.016449072649254184, 'neutral': 3.430695050275916}, 'dominant_emotion': 'fear', 'region': {'x': 259, 'y': 243, 'w': 121, 'h': 121, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.97}]\n",
            "[17/Jun/2024 08:06:15] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            "Received timestamp: 54686\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 3.212700401222658, 'disgust': 0.03205444572682581, 'fear': 84.29692574776104, 'happy': 0.024045167127362085, 'sad': 10.29046427721211, 'surprise': 0.041513096365447776, 'neutral': 2.1022872355833964}, 'dominant_emotion': 'fear', 'region': {'x': 258, 'y': 241, 'w': 121, 'h': 121, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.98}]\n",
            "[17/Jun/2024 08:06:17] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            "Received timestamp: 56686\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 2.329748215347316, 'disgust': 0.0028706419254929446, 'fear': 71.07082856545789, 'happy': 0.059576712200249696, 'sad': 8.419088532815996, 'surprise': 0.03718347731929747, 'neutral': 18.0807165003677}, 'dominant_emotion': 'fear', 'region': {'x': 253, 'y': 239, 'w': 122, 'h': 122, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.98}]\n",
            "[17/Jun/2024 08:06:19] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            "Received timestamp: 58693\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 8.301963657140732, 'disgust': 0.0007240021204779623, 'fear': 26.95474624633789, 'happy': 0.02498037356417626, 'sad': 10.052695870399475, 'surprise': 0.045676183071918786, 'neutral': 54.61921691894531}, 'dominant_emotion': 'neutral', 'region': {'x': 248, 'y': 238, 'w': 128, 'h': 128, 'left_eye': None, 'right_eye': None}, 'face_confidence': 1.0}]\n",
            "[17/Jun/2024 08:06:21] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 60689\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 19.830884039402008, 'disgust': 0.027345152921043336, 'fear': 5.40890097618103, 'happy': 0.5466387141495943, 'sad': 16.407586634159088, 'surprise': 0.602687057107687, 'neutral': 57.17595815658569}, 'dominant_emotion': 'neutral', 'region': {'x': 241, 'y': 236, 'w': 134, 'h': 134, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.99}]\n",
            "[17/Jun/2024 08:06:23] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 62682\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.18955528503283858, 'disgust': 0.004447619721759111, 'fear': 0.9490101598203182, 'happy': 80.01297116279602, 'sad': 3.23924757540226, 'surprise': 0.309766479767859, 'neutral': 15.29499888420105}, 'dominant_emotion': 'happy', 'region': {'x': 233, 'y': 246, 'w': 120, 'h': 120, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.98}]\n",
            "[17/Jun/2024 08:06:25] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 64687\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 1.412066351622343, 'disgust': 0.28584597166627645, 'fear': 7.432623952627182, 'happy': 54.456549882888794, 'sad': 4.846909269690514, 'surprise': 0.9628753177821636, 'neutral': 30.60312569141388}, 'dominant_emotion': 'happy', 'region': {'x': 249, 'y': 236, 'w': 123, 'h': 123, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.99}]\n",
            "[17/Jun/2024 08:06:27] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 66685\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.030165779067930096, 'disgust': 0.0009863990005312283, 'fear': 0.08685113990963818, 'happy': 97.01525008496641, 'sad': 0.21210871436641046, 'surprise': 0.17389694949722892, 'neutral': 2.480735892863763}, 'dominant_emotion': 'happy', 'region': {'x': 229, 'y': 245, 'w': 116, 'h': 116, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.98}]\n",
            "[17/Jun/2024 08:06:29] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 68683\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.1791479987702071, 'disgust': 4.321713378297143e-06, 'fear': 0.11486364049024227, 'happy': 9.607196426820542, 'sad': 0.5921188629191436, 'surprise': 0.08288290612819231, 'neutral': 89.42378696345013}, 'dominant_emotion': 'neutral', 'region': {'x': 214, 'y': 228, 'w': 129, 'h': 129, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.98}]\n",
            "[17/Jun/2024 08:06:31] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 70682\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.22111167199909687, 'disgust': 6.373789274149999e-08, 'fear': 99.6956467628479, 'happy': 1.2650905034661264e-05, 'sad': 0.06685577100142837, 'surprise': 0.00018624547237777733, 'neutral': 0.01618645910639316}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:06:33] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            "Received timestamp: 72702\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.21450945641845465, 'disgust': 3.7236036476429035e-08, 'fear': 99.70115423202515, 'happy': 6.697870276184403e-05, 'sad': 0.06556000444106758, 'surprise': 0.0004336345227784477, 'neutral': 0.018274666217621416}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:06:35] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            "Received timestamp: 74701\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 1.8824700266122818, 'disgust': 0.06328503950498998, 'fear': 0.571047468110919, 'happy': 94.45406198501587, 'sad': 2.5994494557380676, 'surprise': 0.018885811732616276, 'neutral': 0.41080317460000515}, 'dominant_emotion': 'happy', 'region': {'x': 201, 'y': 224, 'w': 143, 'h': 143, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.92}]\n",
            "[17/Jun/2024 08:06:37] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 76684\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.0338773816294525, 'disgust': 0.0004582186466266805, 'fear': 0.0063932446382638595, 'happy': 98.71239058449213, 'sad': 0.18858292565616522, 'surprise': 0.00740390504254021, 'neutral': 1.050898122841008}, 'dominant_emotion': 'happy', 'region': {'x': 222, 'y': 216, 'w': 139, 'h': 139, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.93}]\n",
            "[17/Jun/2024 08:06:39] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 78682\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.04926305841126342, 'disgust': 0.00036622903007594557, 'fear': 0.016636275740393162, 'happy': 98.64436404176188, 'sad': 0.12069681850337166, 'surprise': 0.01927777836792578, 'neutral': 1.1493934810884618}, 'dominant_emotion': 'happy', 'region': {'x': 206, 'y': 221, 'w': 137, 'h': 137, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.92}]\n",
            "[17/Jun/2024 08:06:41] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 80691\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.001578352203068789, 'disgust': 1.9704833320588477e-06, 'fear': 0.00012415931678333436, 'happy': 99.37949180603027, 'sad': 0.01159950697910972, 'surprise': 0.00316506193485111, 'neutral': 0.6040448788553476}, 'dominant_emotion': 'happy', 'region': {'x': 210, 'y': 217, 'w': 137, 'h': 137, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.92}]\n",
            "[17/Jun/2024 08:06:43] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 82682\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 35.60531053574744, 'disgust': 0.007179598897457429, 'fear': 11.344336649437944, 'happy': 0.13109363698399884, 'sad': 32.42690458458093, 'surprise': 0.32227944640990774, 'neutral': 20.162886092834835}, 'dominant_emotion': 'angry', 'region': {'x': 213, 'y': 219, 'w': 138, 'h': 138, 'left_eye': (296, 266), 'right_eye': (254, 273)}, 'face_confidence': 0.93}]\n",
            "[17/Jun/2024 08:06:45] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 84693\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 20.569351315498352, 'disgust': 0.045910567860119045, 'fear': 50.652796030044556, 'happy': 0.9014688432216644, 'sad': 20.319131016731262, 'surprise': 0.8939393796026707, 'neutral': 6.617400050163269}, 'dominant_emotion': 'fear', 'region': {'x': 223, 'y': 233, 'w': 126, 'h': 126, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.96}]\n",
            "[17/Jun/2024 08:06:47] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            "Received timestamp: 86689\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 74.73921179771423, 'disgust': 0.6236820481717587, 'fear': 14.027507603168488, 'happy': 0.14896240318194032, 'sad': 8.08558315038681, 'surprise': 0.5254078656435013, 'neutral': 1.8496455624699593}, 'dominant_emotion': 'angry', 'region': {'x': 258, 'y': 216, 'w': 134, 'h': 134, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.95}]\n",
            "[17/Jun/2024 08:06:49] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 88681\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 7.50008016825678, 'disgust': 1.5227395062434084, 'fear': 13.532631222323324, 'happy': 60.13391734765975, 'sad': 13.88296650445062, 'surprise': 1.4474243719785658, 'neutral': 1.9802444181137584}, 'dominant_emotion': 'happy', 'region': {'x': 235, 'y': 241, 'w': 121, 'h': 121, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.96}]\n",
            "[17/Jun/2024 08:06:51] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 90681\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 8.430977165699005, 'disgust': 0.7926316931843758, 'fear': 9.073944389820099, 'happy': 79.36225533485413, 'sad': 0.3670488949865103, 'surprise': 1.5192903578281403, 'neutral': 0.45385081321001053}, 'dominant_emotion': 'happy', 'region': {'x': 271, 'y': 229, 'w': 122, 'h': 122, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.96}]\n",
            "[17/Jun/2024 08:06:53] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 92684\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 3.6641977727413177, 'disgust': 0.0029170945708756335, 'fear': 17.49579906463623, 'happy': 5.034950748085976, 'sad': 11.973083764314651, 'surprise': 0.7638088893145323, 'neutral': 61.06524467468262}, 'dominant_emotion': 'neutral', 'region': {'x': 211, 'y': 236, 'w': 128, 'h': 128, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.98}]\n",
            "[17/Jun/2024 08:06:55] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 94683\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 26.91923677921295, 'disgust': 0.2007229719310999, 'fear': 25.026211142539978, 'happy': 24.14628714323044, 'sad': 7.21171647310257, 'surprise': 1.8417676910758018, 'neutral': 14.6540567278862}, 'dominant_emotion': 'angry', 'region': {'x': 173, 'y': 202, 'w': 164, 'h': 164, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.92}]\n",
            "[17/Jun/2024 08:06:57] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:125: UserWarning: Decorating classes is deprecated and will be disabled in future versions. You should only decorate functions or methods. To preserve the current behavior of class decoration, you can directly decorate the `__init__` method and nothing else.\n",
            "  warnings.warn(\"Decorating classes is deprecated and will be disabled in \"\n",
            "{'avator_1': {'audio_clips': {'audio_0': '/content/Human_TalkingHead/media/audio/session-1u4hv44lc_response.wav'}, 'bbox_shift': 5, 'preparation': False, 'video_path': '/content/Human_TalkingHead/media/motion_video/vox/download1_wQl0uhL_jessie.mp4'}}\n",
            "reading images...\n",
            " 21% 176/850 [00:00<00:01, 368.56it/s]Received timestamp: 96684\n",
            "Analyzing frame...\n",
            " 33% 280/850 [00:01<00:02, 217.53it/s]Analysis result: [{'emotion': {'angry': 3.6302488297224045, 'disgust': 0.0017902446415973827, 'fear': 10.33032014966011, 'happy': 2.20829788595438, 'sad': 43.261897563934326, 'surprise': 0.3480825340375304, 'neutral': 40.21936655044556}, 'dominant_emotion': 'sad', 'region': {'x': 178, 'y': 201, 'w': 146, 'h': 146, 'left_eye': (270, 251), 'right_eye': (220, 254)}, 'face_confidence': 0.92}]\n",
            "[17/Jun/2024 08:06:59] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 17\u001b[0m\n",
            " 81% 689/850 [00:02<00:00, 287.20it/s]Received timestamp: 98685\n",
            " 85% 719/850 [00:02<00:00, 245.85it/s]Analyzing frame...\n",
            " 97% 823/850 [00:03<00:00, 158.64it/s]Analysis result: [{'emotion': {'angry': 0.45580537989735603, 'disgust': 0.0006516683697554981, 'fear': 5.091801285743713, 'happy': 73.74107241630554, 'sad': 7.001917809247971, 'surprise': 0.8445292711257935, 'neutral': 12.864217162132263}, 'dominant_emotion': 'happy', 'region': {'x': 185, 'y': 206, 'w': 143, 'h': 143, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.94}]\n",
            "[17/Jun/2024 08:07:01] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "100% 850/850 [00:03<00:00, 240.69it/s]\n",
            "reading images...\n",
            "100% 850/850 [00:00<00:00, 1821.42it/s]\n",
            "Inferring using: /content/Human_TalkingHead/media/audio/session-1u4hv44lc_response.wav\n",
            "start inference\n",
            "Received timestamp: 100696\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 26.46591365337372, 'disgust': 1.2072858623923821e-05, 'fear': 11.825279891490936, 'happy': 0.28910660184919834, 'sad': 9.709058701992035, 'surprise': 51.186615228652954, 'neutral': 0.5240149330347776}, 'dominant_emotion': 'surprise', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:07:03] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 22\u001b[0m\n",
            "Received timestamp: 102682\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 34.88083599577379, 'disgust': 1.3936508245647936, 'fear': 32.39454119140401, 'happy': 0.21605140311138668, 'sad': 21.67099525582734, 'surprise': 0.551651577103326, 'neutral': 8.892277198109293}, 'dominant_emotion': 'angry', 'region': {'x': 181, 'y': 216, 'w': 150, 'h': 150, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.94}]\n",
            "[17/Jun/2024 08:07:05] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "video in 25 FPS, audio idx in 50FPS\n",
            "processing audio:/content/Human_TalkingHead/media/audio/session-1u4hv44lc_response.wav costs 3613.9442920684814ms\n",
            "303\n",
            "  0% 0/76 [00:00<?, ?it/s]Received timestamp: 104681\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 22.307533025741577, 'disgust': 0.7980582304298878, 'fear': 26.120352745056152, 'happy': 0.28175292536616325, 'sad': 39.67909812927246, 'surprise': 0.12651662109419703, 'neutral': 10.686687380075455}, 'dominant_emotion': 'sad', 'region': {'x': 181, 'y': 216, 'w': 147, 'h': 147, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.93}]\n",
            "[17/Jun/2024 08:07:07] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 17\u001b[0m\n",
            "Received timestamp: 106684\n",
            "Analyzing frame...\n",
            "  1% 1/76 [00:03<04:12,  3.37s/it]Analysis result: [{'emotion': {'angry': 27.823426982947947, 'disgust': 6.571970096591699, 'fear': 26.026829954763656, 'happy': 9.346019852635878, 'sad': 16.5902937543042, 'surprise': 1.7413969869736448, 'neutral': 11.900059950344568}, 'dominant_emotion': 'angry', 'region': {'x': 198, 'y': 218, 'w': 140, 'h': 140, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.97}]\n",
            "[17/Jun/2024 08:07:09] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "  7% 5/76 [00:04<00:44,  1.61it/s]Received timestamp: 108682\n",
            "Analyzing frame...\n",
            "  8% 6/76 [00:05<00:36,  1.94it/s]Analysis result: [{'emotion': {'angry': 1.604618786550688, 'disgust': 0.4449856267115847, 'fear': 9.649357774815426, 'happy': 84.09016633571186, 'sad': 2.385254160180936, 'surprise': 0.308754755892143, 'neutral': 1.516866308710951}, 'dominant_emotion': 'happy', 'region': {'x': 216, 'y': 219, 'w': 133, 'h': 133, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.98}]\n",
            "[17/Jun/2024 08:07:11] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            " 14% 11/76 [00:06<00:22,  2.84it/s]Received timestamp: 110684\n",
            "Analyzing frame...\n",
            " 16% 12/76 [00:07<00:21,  2.94it/s]Analysis result: [{'emotion': {'angry': 0.025389765505678952, 'disgust': 0.0068851004471071064, 'fear': 1.1537468060851097, 'happy': 72.67637252807617, 'sad': 3.4556329250335693, 'surprise': 0.019545743998605758, 'neutral': 22.66242802143097}, 'dominant_emotion': 'happy', 'region': {'x': 226, 'y': 211, 'w': 141, 'h': 141, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.99}]\n",
            "[17/Jun/2024 08:07:13] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            " 22% 17/76 [00:08<00:19,  3.06it/s]Received timestamp: 112685\n",
            "Analyzing frame...\n",
            " 24% 18/76 [00:09<00:18,  3.08it/s]Analysis result: [{'emotion': {'angry': 9.273282246189252, 'disgust': 0.5712103904760116, 'fear': 8.075000081956391, 'happy': 75.24210663092772, 'sad': 2.353460481969147, 'surprise': 2.345922356600902, 'neutral': 2.1390219562662813}, 'dominant_emotion': 'happy', 'region': {'x': 202, 'y': 228, 'w': 135, 'h': 135, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.97}]\n",
            "[17/Jun/2024 08:07:15] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            " 30% 23/76 [00:10<00:16,  3.12it/s]Received timestamp: 114685\n",
            "Analyzing frame...\n",
            " 33% 25/76 [00:11<00:20,  2.51it/s]Analysis result: [{'emotion': {'angry': 0.13278344040563347, 'disgust': 0.08423977335108956, 'fear': 1.1290726924879964, 'happy': 93.4317346472582, 'sad': 0.6568917464847414, 'surprise': 0.021469437167089413, 'neutral': 4.5438124712593835}, 'dominant_emotion': 'happy', 'region': {'x': 194, 'y': 222, 'w': 143, 'h': 143, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.97}]\n",
            "[17/Jun/2024 08:07:17] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            " 37% 28/76 [00:12<00:17,  2.73it/s]Received timestamp: 116685\n",
            "Analyzing frame...\n",
            " 39% 30/76 [00:13<00:17,  2.65it/s]Analysis result: [{'emotion': {'angry': 0.21625172812491655, 'disgust': 0.028612097958102822, 'fear': 2.573731355369091, 'happy': 87.15236783027649, 'sad': 5.702558532357216, 'surprise': 0.06104906788095832, 'neutral': 4.265427216887474}, 'dominant_emotion': 'happy', 'region': {'x': 157, 'y': 219, 'w': 143, 'h': 143, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.95}]\n",
            "[17/Jun/2024 08:07:19] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            " 43% 33/76 [00:14<00:17,  2.50it/s]Received timestamp: 118691\n",
            "Analyzing frame...\n",
            " 46% 35/76 [00:15<00:17,  2.30it/s]Analysis result: [{'emotion': {'angry': 0.00899452245091193, 'disgust': 0.00010117048463087689, 'fear': 0.028712863637836143, 'happy': 99.10332571228436, 'sad': 0.11118023960751419, 'surprise': 0.006273622400570971, 'neutral': 0.7414069539866502}, 'dominant_emotion': 'happy', 'region': {'x': 171, 'y': 191, 'w': 162, 'h': 162, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.94}]\n",
            "[17/Jun/2024 08:07:21] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            " 50% 38/76 [00:16<00:15,  2.44it/s]Received timestamp: 120692\n",
            "Analyzing frame...\n",
            " 51% 39/76 [00:17<00:16,  2.20it/s]Analysis result: [{'emotion': {'angry': 0.1180532480489316, 'disgust': 0.0025991336400023606, 'fear': 0.4773723323795399, 'happy': 97.7529886063351, 'sad': 0.30512927187928296, 'surprise': 0.4382628890707625, 'neutral': 0.9055942148751956}, 'dominant_emotion': 'happy', 'region': {'x': 152, 'y': 241, 'w': 129, 'h': 129, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.96}]\n",
            "[17/Jun/2024 08:07:23] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            " 55% 42/76 [00:18<00:14,  2.39it/s]Received timestamp: 122694\n",
            "Analyzing frame...\n",
            " 58% 44/76 [00:19<00:13,  2.41it/s]Analysis result: [{'emotion': {'angry': 12.521713974680182, 'disgust': 0.03964526585731099, 'fear': 3.954005340219181, 'happy': 41.01489377199337, 'sad': 2.622929714373316, 'surprise': 3.0088253180772444, 'neutral': 36.83799050307161}, 'dominant_emotion': 'happy', 'region': {'x': 208, 'y': 218, 'w': 141, 'h': 141, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.95}]\n",
            "[17/Jun/2024 08:07:25] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            " 62% 47/76 [00:20<00:11,  2.44it/s]Received timestamp: 124687\n",
            "Analyzing frame...\n",
            " 63% 48/76 [00:21<00:11,  2.40it/s]Analysis result: [{'emotion': {'angry': 60.219353437423706, 'disgust': 1.1527035385370255, 'fear': 10.96409186720848, 'happy': 10.899700224399567, 'sad': 7.280193269252777, 'surprise': 7.7355630695819855, 'neutral': 1.7483988776803017}, 'dominant_emotion': 'angry', 'region': {'x': 235, 'y': 227, 'w': 131, 'h': 131, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.99}]\n",
            "[17/Jun/2024 08:07:27] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            " 68% 52/76 [00:22<00:08,  2.74it/s]Received timestamp: 126697\n",
            " 70% 53/76 [00:23<00:08,  2.82it/s]Analyzing frame...\n",
            " 71% 54/76 [00:23<00:08,  2.68it/s]Analysis result: [{'emotion': {'angry': 12.96473890542984, 'disgust': 0.0018757364159682766, 'fear': 26.713868975639343, 'happy': 0.17134274821728468, 'sad': 16.481372714042664, 'surprise': 0.04596959624905139, 'neutral': 43.62083673477173}, 'dominant_emotion': 'neutral', 'region': {'x': 177, 'y': 256, 'w': 117, 'h': 117, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.96}]\n",
            "[17/Jun/2024 08:07:29] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            " 76% 58/76 [00:24<00:06,  2.78it/s]Received timestamp: 128699\n",
            "Analyzing frame...\n",
            " 79% 60/76 [00:25<00:07,  2.27it/s]Analysis result: [{'emotion': {'angry': 11.454846647670665, 'disgust': 9.384892312000536e-08, 'fear': 84.821896840447, 'happy': 0.0033943719449358103, 'sad': 1.5656446108606226, 'surprise': 0.8899928601081883, 'neutral': 1.264224649323502}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:07:31] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            " 83% 63/76 [00:26<00:05,  2.54it/s]Received timestamp: 130683\n",
            "Analyzing frame...\n",
            " 84% 64/76 [00:27<00:04,  2.52it/s]Analysis result: [{'emotion': {'angry': 0.015827224274738965, 'disgust': 5.513767202773584e-05, 'fear': 96.77352347689587, 'happy': 0.0002185417509784162, 'sad': 3.156540166552047, 'surprise': 0.04522608694253571, 'neutral': 0.008598074485265152}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:07:33] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            " 89% 68/76 [00:28<00:02,  2.78it/s]Received timestamp: 132684\n",
            "Analyzing frame...\n",
            " 92% 70/76 [00:29<00:02,  2.57it/s]Analysis result: [{'emotion': {'angry': 21.331195533275604, 'disgust': 1.6270514577627182, 'fear': 31.809893250465393, 'happy': 16.862323880195618, 'sad': 14.074079692363739, 'surprise': 0.8326791226863861, 'neutral': 13.46278190612793}, 'dominant_emotion': 'fear', 'region': {'x': 201, 'y': 207, 'w': 149, 'h': 149, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.92}]\n",
            "[17/Jun/2024 08:07:35] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            " 96% 73/76 [00:30<00:01,  2.59it/s]Received timestamp: 134683\n",
            "Analyzing frame...\n",
            " 97% 74/76 [00:31<00:00,  2.73it/s]Analysis result: [{'emotion': {'angry': 47.824608890642324, 'disgust': 2.2615587657497107, 'fear': 35.22130880736866, 'happy': 1.5870233110231122, 'sad': 9.93379755624489, 'surprise': 0.5895441520889078, 'neutral': 2.5821655018029515}, 'dominant_emotion': 'angry', 'region': {'x': 184, 'y': 237, 'w': 140, 'h': 140, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.92}]\n",
            "[17/Jun/2024 08:07:37] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            " 99% 75/76 [00:31<00:00,  2.69it/s]Received timestamp: 136685\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 24.139009416103363, 'disgust': 0.009271593444282189, 'fear': 4.884853959083557, 'happy': 0.26501845568418503, 'sad': 8.747094124555588, 'surprise': 0.8832276798784733, 'neutral': 61.071521043777466}, 'dominant_emotion': 'neutral', 'region': {'x': 241, 'y': 214, 'w': 137, 'h': 137, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.99}]\n",
            "[17/Jun/2024 08:07:39] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "100% 76/76 [00:33<00:00,  2.26it/s]\n",
            "Total process time of 303 frames including saving images = 33.60555672645569s\n",
            "ffmpeg -y -v warning -r 25 -f image2 -i ./results/avatars/avator_1/tmp/%08d.png -vcodec libx264 -vf format=rgb24,scale=out_color_matrix=bt709,format=yuv420p -crf 18 ./results/avatars/avator_1/temp.mp4\n",
            "Received timestamp: 138697\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.058469653595238924, 'disgust': 2.8053945277939363e-08, 'fear': 0.05826355773024261, 'happy': 0.003724572161445394, 'sad': 0.1395621569827199, 'surprise': 0.0017195094187627546, 'neutral': 99.73825812339783}, 'dominant_emotion': 'neutral', 'region': {'x': 228, 'y': 236, 'w': 126, 'h': 126, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.99}]\n",
            "[17/Jun/2024 08:07:41] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "ffmpeg -y -v warning -i /content/Human_TalkingHead/media/audio/session-1u4hv44lc_response.wav -i ./results/avatars/avator_1/temp.mp4 ./results/avatars/avator_1/vid_output/audio_0.mp4\n",
            "\u001b[0;33mGuessed Channel Layout for Input Stream #0.0 : mono\n",
            "\u001b[0mReceived timestamp: 140731\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 8.943642996983886, 'disgust': 0.7553501023769501, 'fear': 23.490625439956837, 'happy': 14.45411616016819, 'sad': 14.672179779074165, 'surprise': 0.7884940141134279, 'neutral': 36.89559839911442}, 'dominant_emotion': 'neutral', 'region': {'x': 231, 'y': 229, 'w': 131, 'h': 131, 'left_eye': None, 'right_eye': None}, 'face_confidence': 1.0}]\n",
            "[17/Jun/2024 08:07:43] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "result is save to ./results/avatars/avator_1/vid_output/audio_0.mp4\n",
            "Received timestamp: 142696\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 1.3408494181931019, 'disgust': 0.012982319458387792, 'fear': 3.182213380932808, 'happy': 5.025256797671318, 'sad': 6.242517381906509, 'surprise': 0.1522162463515997, 'neutral': 84.04396772384644}, 'dominant_emotion': 'neutral', 'region': {'x': 227, 'y': 232, 'w': 127, 'h': 127, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.99}]\n",
            "[17/Jun/2024 08:07:45] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 144688\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 58.298128843307495, 'disgust': 0.20776798482984304, 'fear': 9.166605025529861, 'happy': 0.03332128981128335, 'sad': 22.929422557353973, 'surprise': 0.241665611974895, 'neutral': 9.12308469414711}, 'dominant_emotion': 'angry', 'region': {'x': 176, 'y': 222, 'w': 143, 'h': 143, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.92}]\n",
            "[17/Jun/2024 08:07:47] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "[17/Jun/2024 08:07:48] \u001b[m\"POST /api/speech-analysis/ HTTP/1.1\" 200 402\u001b[0m\n",
            "Received timestamp: 146698\n",
            "Analyzing frame...\n",
            "Received timestamp: 147875\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.15546464128419757, 'disgust': 7.625438475100665e-09, 'fear': 0.0010548340469540562, 'happy': 0.011167155025759712, 'sad': 0.14220576267689466, 'surprise': 0.00039514047784905415, 'neutral': 99.68971014022827}, 'dominant_emotion': 'neutral', 'region': {'x': 168, 'y': 224, 'w': 133, 'h': 133, 'left_eye': (261, 261), 'right_eye': (215, 268)}, 'face_confidence': 0.95}]\n",
            "[17/Jun/2024 08:07:49] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Analysis result: [{'emotion': {'angry': 9.690575301647186, 'disgust': 0.029138982063159347, 'fear': 34.31937396526337, 'happy': 0.4722359124571085, 'sad': 7.581162452697754, 'surprise': 0.8648787625133991, 'neutral': 47.04262912273407}, 'dominant_emotion': 'neutral', 'region': {'x': 168, 'y': 224, 'w': 133, 'h': 133, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.95}]\n",
            "[17/Jun/2024 08:07:49] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 148893\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 55.204617356703594, 'disgust': 0.08073714181375304, 'fear': 20.8699559128337, 'happy': 0.015655475315730236, 'sad': 12.448325759372025, 'surprise': 0.2510301003867268, 'neutral': 11.129675223866078}, 'dominant_emotion': 'angry', 'region': {'x': 189, 'y': 215, 'w': 139, 'h': 139, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.93}]\n",
            "[17/Jun/2024 08:07:50] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 149902\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 21.57800752854437, 'disgust': 0.3562826156655092, 'fear': 18.064544268196137, 'happy': 0.05967968156468052, 'sad': 50.2633154706139, 'surprise': 0.1626756352581841, 'neutral': 9.515498967826243}, 'dominant_emotion': 'sad', 'region': {'x': 201, 'y': 207, 'w': 143, 'h': 143, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.93}]\n",
            "[17/Jun/2024 08:07:51] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 17\u001b[0m\n",
            "Received timestamp: 150905\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 6.283362274928095e-07, 'disgust': 1.0335028460210088e-12, 'fear': 99.99829530715942, 'happy': 2.659755193867852e-08, 'sad': 0.000936728702072287, 'surprise': 0.0007660142728127539, 'neutral': 5.282663906314156e-06}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:07:52] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            "Received timestamp: 151914\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.02050328184850514, 'disgust': 2.694001466352347e-05, 'fear': 97.62012958526611, 'happy': 0.0003278952817709069, 'sad': 2.003357745707035, 'surprise': 0.33741937950253487, 'neutral': 0.018237474432680756}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:07:53] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            "[17/Jun/2024 08:07:53] \u001b[m\"HEAD /videos/audio_0.mp4 HTTP/1.1\" 200 0\u001b[0m\n",
            "Received timestamp: 152918\n",
            "Analyzing frame...\n",
            "[17/Jun/2024 08:07:54] \u001b[m\"GET /videos/audio_0.mp4?t=1718611674556 HTTP/1.1\" 200 305010\u001b[0m\n",
            "Analysis result: [{'emotion': {'angry': 6.828184494755804, 'disgust': 0.0010189327745625239, 'fear': 4.371964707608784, 'happy': 0.04647062131971077, 'sad': 12.383624152377147, 'surprise': 0.09111717148963171, 'neutral': 76.27762273283275}, 'dominant_emotion': 'neutral', 'region': {'x': 183, 'y': 223, 'w': 140, 'h': 140, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.95}]\n",
            "[17/Jun/2024 08:07:54] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 153923\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.29766447842121124, 'disgust': 3.361408218438555e-06, 'fear': 97.73008227348328, 'happy': 1.0498798275193622e-05, 'sad': 1.608564518392086, 'surprise': 0.0004988871751265833, 'neutral': 0.36317603662610054}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:07:55] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            "Received timestamp: 154927\n",
            "Analyzing frame...\n",
            "Not Found: /favicon.ico\n",
            "WARNING:django.request:Not Found: /favicon.ico\n",
            "[17/Jun/2024 08:07:56] \u001b[33m\"GET /favicon.ico HTTP/1.1\" 404 3099\u001b[0m\n",
            "Analysis result: [{'emotion': {'angry': 0.06292518642319006, 'disgust': 1.0178815465009551e-07, 'fear': 98.94486678303876, 'happy': 3.0510310637921577e-06, 'sad': 0.6836479180427888, 'surprise': 8.746595293420125e-05, 'neutral': 0.30846461701814926}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:07:56] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            "Received timestamp: 155931\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 1.024728175252676, 'disgust': 3.166629403139609e-07, 'fear': 98.72046709060669, 'happy': 1.8385444278123941e-07, 'sad': 0.23599604610353708, 'surprise': 1.1958925938415632e-05, 'neutral': 0.018797973461914808}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:07:57] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            "Received timestamp: 156936\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.23609716445207596, 'disgust': 6.785284001331604e-08, 'fear': 0.8692000992596149, 'happy': 0.0028771599318133667, 'sad': 1.1036562733352184, 'surprise': 0.005804982356494293, 'neutral': 97.78236150741577}, 'dominant_emotion': 'neutral', 'region': {'x': 198, 'y': 209, 'w': 134, 'h': 134, 'left_eye': (279, 255), 'right_eye': (237, 261)}, 'face_confidence': 0.96}]\n",
            "[17/Jun/2024 08:07:58] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 157940\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 4.201759025454521, 'disgust': 0.00015151903198784566, 'fear': 0.3154269652441144, 'happy': 0.007506862311856821, 'sad': 0.3735253820195794, 'surprise': 0.13175110798329115, 'neutral': 94.9698805809021}, 'dominant_emotion': 'neutral', 'region': {'x': 179, 'y': 211, 'w': 140, 'h': 140, 'left_eye': (264, 260), 'right_eye': (222, 266)}, 'face_confidence': 0.94}]\n",
            "[17/Jun/2024 08:07:59] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 158945\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.0016895590306376107, 'disgust': 6.32875522821319e-10, 'fear': 96.4583694934845, 'happy': 3.177493379524776e-07, 'sad': 3.519763797521591, 'surprise': 1.2794732287546573e-09, 'neutral': 0.020182100706733763}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:08:00] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            "Received timestamp: 159950\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 70.09695528781947, 'disgust': 0.06311671288782801, 'fear': 5.481100174057463, 'happy': 0.20264035469110553, 'sad': 8.951984023811338, 'surprise': 0.09254593812312924, 'neutral': 15.111657665770332}, 'dominant_emotion': 'angry', 'region': {'x': 231, 'y': 232, 'w': 114, 'h': 114, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.96}]\n",
            "[17/Jun/2024 08:08:01] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 160953\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 1.059192605316639, 'disgust': 0.0003587082574085798, 'fear': 88.95019292831421, 'happy': 0.005242173210717738, 'sad': 9.617714583873749, 'surprise': 8.619218192507105e-05, 'neutral': 0.3672181395813823}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:08:02] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            "Received timestamp: 161957\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.0007041476202603175, 'disgust': 1.8693513629930847e-06, 'fear': 96.08843303253848, 'happy': 0.001152558048064788, 'sad': 3.6914745683356864, 'surprise': 6.816642590352321e-06, 'neutral': 0.21822777480499594}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:08:03] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            "Received timestamp: 162986\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 44.29423213005066, 'disgust': 0.12498424621298909, 'fear': 46.17499113082886, 'happy': 0.2249183366075158, 'sad': 3.9029981940984726, 'surprise': 0.5167483352124691, 'neutral': 4.76112999022007}, 'dominant_emotion': 'fear', 'region': {'x': 233, 'y': 253, 'w': 111, 'h': 111, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.95}]\n",
            "[17/Jun/2024 08:08:04] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            "Received timestamp: 163989\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 7.418846199650026, 'disgust': 0.0021418594035478274, 'fear': 37.929104217397665, 'happy': 0.42531340900797737, 'sad': 1.9051841982979538, 'surprise': 0.9492963352148134, 'neutral': 51.3701079642644}, 'dominant_emotion': 'neutral', 'region': {'x': 246, 'y': 230, 'w': 118, 'h': 118, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.93}]\n",
            "[17/Jun/2024 08:08:05] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 164995\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.000241919997279183, 'disgust': 1.0854432908047329e-07, 'fear': 23.00066649913788, 'happy': 1.1522056553303628e-05, 'sad': 76.97773575782776, 'surprise': 1.996863296938045e-06, 'neutral': 0.02134694514097646}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:08:06] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 17\u001b[0m\n",
            "Received timestamp: 165999\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 25.041940808296204, 'disgust': 1.6279321163892746, 'fear': 57.35386610031128, 'happy': 1.1567534878849983, 'sad': 12.616021931171417, 'surprise': 0.21722461096942425, 'neutral': 1.9862689077854156}, 'dominant_emotion': 'fear', 'region': {'x': 216, 'y': 242, 'w': 122, 'h': 122, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.93}]\n",
            "[17/Jun/2024 08:08:07] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            "Received timestamp: 167003\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 4.312829300761223, 'disgust': 0.16030268743634224, 'fear': 10.165529698133469, 'happy': 18.66060048341751, 'sad': 5.53075410425663, 'surprise': 2.3817310109734535, 'neutral': 58.788251876831055}, 'dominant_emotion': 'neutral', 'region': {'x': 254, 'y': 217, 'w': 119, 'h': 119, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.95}]\n",
            "[17/Jun/2024 08:08:08] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 168006\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 28.201615428731987, 'disgust': 0.2608881952299839, 'fear': 46.61236445756382, 'happy': 5.73181702413356, 'sad': 12.758485942067392, 'surprise': 3.126941621072099, 'neutral': 3.3078863998786927}, 'dominant_emotion': 'fear', 'region': {'x': 216, 'y': 239, 'w': 118, 'h': 118, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.93}]\n",
            "[17/Jun/2024 08:08:09] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            "Received timestamp: 169015\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 15.045198204827201, 'disgust': 0.12526841183861565, 'fear': 51.4688045661929, 'happy': 13.60048732760473, 'sad': 11.296692418854978, 'surprise': 1.6538624706186336, 'neutral': 6.809688765388057}, 'dominant_emotion': 'fear', 'region': {'x': 222, 'y': 232, 'w': 123, 'h': 123, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.95}]\n",
            "[17/Jun/2024 08:08:10] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            "Received timestamp: 170019\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 28.87599766254425, 'disgust': 3.483722358942032, 'fear': 45.85171937942505, 'happy': 1.4424027875065804, 'sad': 18.529722094535828, 'surprise': 0.251305871643126, 'neutral': 1.5651358291506767}, 'dominant_emotion': 'fear', 'region': {'x': 212, 'y': 245, 'w': 116, 'h': 116, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.93}]\n",
            "[17/Jun/2024 08:08:11] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 18\u001b[0m\n",
            "Received timestamp: 171028\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 10.97593754529953, 'disgust': 0.002482401760062203, 'fear': 2.2797437384724617, 'happy': 0.05355492467060685, 'sad': 3.018111176788807, 'surprise': 1.5261756256222725, 'neutral': 82.14399218559265}, 'dominant_emotion': 'neutral', 'region': {'x': 223, 'y': 244, 'w': 128, 'h': 128, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.92}]\n",
            "[17/Jun/2024 08:08:12] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 172035\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 56.87799967789654, 'disgust': 0.08074465642188115, 'fear': 5.6985486956180695, 'happy': 1.7149513405619454, 'sad': 5.079351505118429, 'surprise': 1.0770331715603862, 'neutral': 29.47137004478335}, 'dominant_emotion': 'angry', 'region': {'x': 216, 'y': 238, 'w': 123, 'h': 123, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.92}]\n",
            "[17/Jun/2024 08:08:13] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 173038\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 65.20693898200989, 'disgust': 0.09355733636766672, 'fear': 6.221549212932587, 'happy': 0.14378582127392292, 'sad': 3.2034993171691895, 'surprise': 1.7687898129224777, 'neutral': 23.36188107728958}, 'dominant_emotion': 'angry', 'region': {'x': 243, 'y': 228, 'w': 131, 'h': 131, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.92}]\n",
            "[17/Jun/2024 08:08:14] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 174043\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 64.97642815394066, 'disgust': 0.27604616681681077, 'fear': 13.849074977746609, 'happy': 0.6623311110042202, 'sad': 4.009041457290476, 'surprise': 2.7814558185391642, 'neutral': 13.445615655706447}, 'dominant_emotion': 'angry', 'region': {'x': 221, 'y': 241, 'w': 120, 'h': 120, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.93}]\n",
            "[17/Jun/2024 08:08:15] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 175049\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 77.20218572590521, 'disgust': 0.03876300008346884, 'fear': 8.785108919726738, 'happy': 0.1412127464309041, 'sad': 0.7436011497108758, 'surprise': 0.659517306444966, 'neutral': 12.429600217390034}, 'dominant_emotion': 'angry', 'region': {'x': 217, 'y': 248, 'w': 122, 'h': 122, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.91}]\n",
            "[17/Jun/2024 08:08:16] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 176052\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 56.26501441001892, 'disgust': 0.040641461964696646, 'fear': 10.775317251682281, 'happy': 0.4494890570640564, 'sad': 3.023752011358738, 'surprise': 0.6616125348955393, 'neutral': 28.7841796875}, 'dominant_emotion': 'angry', 'region': {'x': 220, 'y': 242, 'w': 123, 'h': 123, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.95}]\n",
            "[17/Jun/2024 08:08:17] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 177056\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 61.8337021072266, 'disgust': 0.11429959942554242, 'fear': 13.592621553823763, 'happy': 0.8297510387300207, 'sad': 3.650540423328826, 'surprise': 4.120668650189799, 'neutral': 15.858411388586594}, 'dominant_emotion': 'angry', 'region': {'x': 226, 'y': 234, 'w': 119, 'h': 119, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.94}]\n",
            "[17/Jun/2024 08:08:18] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 178062\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 78.84896397590637, 'disgust': 0.004116564741707407, 'fear': 9.034344553947449, 'happy': 0.07099336944520473, 'sad': 1.218843273818493, 'surprise': 0.3192352131009102, 'neutral': 10.503491759300232}, 'dominant_emotion': 'angry', 'region': {'x': 208, 'y': 246, 'w': 120, 'h': 120, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.94}]\n",
            "[17/Jun/2024 08:08:19] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 179067\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 58.73618721961975, 'disgust': 0.00727537480997853, 'fear': 12.811113893985748, 'happy': 0.8779151365160942, 'sad': 0.8515149354934692, 'surprise': 2.8243476524949074, 'neutral': 23.89165163040161}, 'dominant_emotion': 'angry', 'region': {'x': 210, 'y': 240, 'w': 126, 'h': 126, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.94}]\n",
            "[17/Jun/2024 08:08:20] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 19\u001b[0m\n",
            "Received timestamp: 180071\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 1.3341061770915985, 'disgust': 7.406142543686656e-06, 'fear': 1.5521137975156307, 'happy': 0.1558294170536101, 'sad': 0.28960409108549356, 'surprise': 0.8222827687859535, 'neutral': 95.84605693817139}, 'dominant_emotion': 'neutral', 'region': {'x': 226, 'y': 229, 'w': 132, 'h': 132, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.92}]\n",
            "[17/Jun/2024 08:08:21] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 181075\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 3.542545437812805, 'disgust': 6.065584443604166e-05, 'fear': 2.0734498277306557, 'happy': 0.01766343630151823, 'sad': 0.7266571279615164, 'surprise': 0.4447376821190119, 'neutral': 93.19489002227783}, 'dominant_emotion': 'neutral', 'region': {'x': 230, 'y': 239, 'w': 122, 'h': 122, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.94}]\n",
            "[17/Jun/2024 08:08:22] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 182080\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 21.107806265354156, 'disgust': 0.0006111547008913476, 'fear': 3.71139720082283, 'happy': 0.05319967167451978, 'sad': 0.6027855444699526, 'surprise': 1.1382241733372211, 'neutral': 73.38597774505615}, 'dominant_emotion': 'neutral', 'region': {'x': 237, 'y': 220, 'w': 132, 'h': 132, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.94}]\n",
            "[17/Jun/2024 08:08:23] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 183086\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 5.344581123540536, 'disgust': 0.0027070045278492195, 'fear': 14.06337935487978, 'happy': 0.01699948391265603, 'sad': 8.121080174574873, 'surprise': 0.024977925999252616, 'neutral': 72.42628007248402}, 'dominant_emotion': 'neutral', 'region': {'x': 232, 'y': 237, 'w': 124, 'h': 124, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.92}]\n",
            "[17/Jun/2024 08:08:24] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 184091\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.1281387056224048, 'disgust': 1.8213255348342727e-07, 'fear': 0.24845951702445745, 'happy': 0.001783157313184347, 'sad': 0.19348071655258536, 'surprise': 0.020916272478643805, 'neutral': 99.40722584724426}, 'dominant_emotion': 'neutral', 'region': {'x': 258, 'y': 225, 'w': 123, 'h': 123, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.94}]\n",
            "[17/Jun/2024 08:08:25] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 185133\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.05277268937788904, 'disgust': 2.734977522234061e-09, 'fear': 0.024175179714802653, 'happy': 0.0011772275684052147, 'sad': 0.10331061203032732, 'surprise': 0.0024046814360190183, 'neutral': 99.81616139411926}, 'dominant_emotion': 'neutral', 'region': {'x': 226, 'y': 230, 'w': 118, 'h': 118, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.96}]\n",
            "[17/Jun/2024 08:08:26] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n",
            "Received timestamp: 186139\n",
            "Analyzing frame...\n",
            "Analysis result: [{'emotion': {'angry': 0.002313310505996924, 'disgust': 9.540306123726066e-15, 'fear': 22.63077050447464, 'happy': 0.34434881526976824, 'sad': 0.3424074035137892, 'surprise': 25.25309920310974, 'neutral': 51.42706036567688}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
            "[17/Jun/2024 08:08:27] \u001b[m\"POST /api/emotion-detection/ HTTP/1.1\" 200 21\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2024-06-17T08:08:27+0000 lvl=warn msg=\"Stopping forwarder\" name=http-7000-ca6ae3b4-d3c3-4b58-a73e-f2b2bb1bd7db acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2024-06-17T08:08:27+0000 lvl=warn msg=\"Error restarting forwarder\" name=http-7000-ca6ae3b4-d3c3-4b58-a73e-f2b2bb1bd7db err=\"failed to start tunnel: session closed\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()\n"
      ],
      "metadata": {
        "id": "k_ruFhHS7z6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GQCMebWv9_nw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
